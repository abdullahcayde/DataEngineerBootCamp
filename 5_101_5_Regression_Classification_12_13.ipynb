{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b26a380-5cba-407e-807b-c825e3e42d54",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694dd042",
   "metadata": {},
   "outputs": [],
   "source": [
    "Introduction\n",
    "\n",
    "    The objective of this exercise is to become familiar with linear regression. Linear regression was one of the first predictive models to be studied and is today one of the most popular models for practical applications thanks to its simplicity.\n",
    "\n",
    "Univariate Linear Regression\n",
    "\n",
    "    In the univariate linear model, we have two variables: ğ‘¦\n",
    "\n",
    "called the target variable and ğ‘¥\n",
    "\n",
    "called the explanatory variable. Linear regression consists in modeling the link between these two variables by an affine function. Thus, the formula of the univariate linear model is given by:\n",
    "\n",
    "ğ‘¦â‰ˆğ›½1ğ‘¥+ğ›½0\n",
    "\n",
    "where:\n",
    "\n",
    "        ğ‘¦\n",
    "\n",
    "is the variable we want to predict.\n",
    "ğ‘¥\n",
    "is the explanatory variable.\n",
    "ğ›½1\n",
    "and ğ›½0 are the parameters of the affine function. ğ›½1 will define its slope and ğ›½0\n",
    "\n",
    "        will define its y-intercept (also called bias).\n",
    "\n",
    "The goal of linear regression is to estimate the best parameters ğ›½0\n",
    "and ğ›½1 to predict the variable ğ‘¦ from a given value of ğ‘¥\n",
    "\n",
    "    .\n",
    "\n",
    "To get a feel for Univariate Linear Regression, let us look at the interactive example below.\n",
    "\n",
    "    (a) Run the next cell to display the interactive figure. In this figure, we have simulated a dataset by the relation ğ‘¦=ğ›¼1ğ‘¥+ğ›¼0\n",
    "\n",
    "    .\n",
    "\n",
    "    (b) Use the sliders on the Regression tab to find the parameters ğ›½0\n",
    "\n",
    "and ğ›½1\n",
    "\n",
    "    that best match all the points in the data set.\n",
    "\n",
    "    (c) What is the effect of each of the parameters on the regression function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widgets import regression_widget\n",
    "\n",
    "regression_widget()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd106324",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multivariate Linear Regression\n",
    "\n",
    "    Multivariate linear regression consists in modeling a linear link between a target variable ğ‘¦\n",
    "\n",
    "and several explanatory variables ğ‘¥1, ğ‘¥2, ..., ğ‘¥ğ‘\n",
    "\n",
    ", often called features:\n",
    "\n",
    "ğ‘¦â‰ˆÎ²0+Î²1ğ‘¥1+Î²2ğ‘¥2+â‹¯+Î²ğ‘ğ‘¥ğ‘â‰ˆÎ²0+âˆ‘ğ‘—=1ğ‘Î²ğ‘—ğ‘¥ğ‘—\n",
    "\n",
    "There are now ğ‘+1\n",
    "parameters ğ›½ğ‘—\n",
    "\n",
    "    to find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc99950",
   "metadata": {},
   "source": [
    "## 1. Using scikit-learn for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "1. Using scikit-learn for linear regression\n",
    "\n",
    "    We are now going to learn how to use the scikit-learn library in order to solve a Machine Learning problem with a linear regression.\n",
    "\n",
    "    During the following exercises, the objective will be to predict the selling price of a car based on its characteristics.\n",
    "\n",
    "Importing the dataset\n",
    "\n",
    "    The dataset that we will use in the following contains many characteristics about different cars from 1985.\n",
    "\n",
    "    For simplicity, only the numeric variables have been kept and the lines containing missing values have been deleted.\n",
    "\n",
    "    (a) Import the pandas module under the alias pd.\n",
    "\n",
    "    (b) In a DataFrame nameddf, import the automobiles.csv dataset using the read_csv function ofpandas. This file is located in the same folder as the runtime environment of the notebook.\n",
    "\n",
    "    (c) Display the first 5 lines of df to check if the import was successful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('automobiles.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716db423",
   "metadata": {},
   "outputs": [],
   "source": [
    "        The symboling variable corresponds to the degree of risk with respect to the insurer (risk of accident, breakdown, etc.).\n",
    "\n",
    "        The normalized_losses variable is the relative average cost per year of vehicle insurance. This value is normalized with respect to cars of the same type (SUV, utility, sports, etc.).\n",
    "\n",
    "        The following 13 variables concern the technical characteristics of the cars such as width, length, engine displacement, horsepower, etc ...\n",
    "\n",
    "        The last variable price corresponds to the selling price of the vehicle. This is the variable that we will try to predict.\n",
    "\n",
    "Separation of the explanatory variables from the target variable\n",
    "\n",
    "    We are now going to create two DataFrames, one containing the explanatory variables and another containing the target variable price.\n",
    "\n",
    "    (d) In a DataFrame named X, make a copy of the explanatory variables of our data set, that is to say all the variables except price.\n",
    "\n",
    "    (e) In a DataFrame named y, make a copy of the target variable price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7281f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "#or\n",
    "X = df.drop(['price'], axis = 1)\n",
    "\n",
    "y = df.iloc[:, -1:]\n",
    "#or\n",
    "y = df['price']\n",
    "\n",
    "\n",
    "# Orginala Cozum\n",
    "# Explanatory variables\n",
    "X = df.drop(['price'], axis = 1)\n",
    "\n",
    "# Target variable\n",
    "y = df['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70966091",
   "metadata": {},
   "source": [
    "## Splitting of the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cf12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Splitting of the data into training and test sets\n",
    "\n",
    "    We are now going to split our dataset into two sets : A training set and a test set. This step is extremely important when doing Machine Learning.\n",
    "\n",
    "    Indeed, as their names indicate:\n",
    "\n",
    "            The training set is used to train the model, ie to find the optimal ğ›½0\n",
    "\n",
    ", ..., ğ›½ğ‘\n",
    "\n",
    "            parameters for this datase t.\n",
    "\n",
    "            The test set is used to test the trained model by evaluating its ability to generalize its predictions on data that it has never seen .\n",
    "\n",
    "    A very useful function for doing this is the train_test_split function of the model_selection submodule of scikit-learn.\n",
    "\n",
    "    (f) Run the following cell to import the train_test_split function.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "    This function is used as follows:\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "            X_train andy_train are the explanatory and target variables of the training dataset.\n",
    "\n",
    "            X_test andy_test are the explanatory and target variables of the test dataset.\n",
    "\n",
    "            The test_size parameter corresponds to the proportion of the dataset that we want to keep for the test set. In the previous example, this proportion corresponds to 20% of the initial dataset.\n",
    "\n",
    "    (g) Using the train_test_split function, separate the dataset into a training set (X_train, y_train) and a test set (X_test, y_test) so that the test set contains 15% of the initial dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdafab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15)\n",
    "\n",
    "#Original Cozum\n",
    "# Splitting the dataset into a training set (85%) and a test set (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294bce20",
   "metadata": {},
   "source": [
    "## Training the regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a3cdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training the regression model\n",
    "\n",
    "    To train a linear regression model on this dataset, we will use the LinearRegression class contained in the linear_model submodule of scikit-learn.\n",
    "\n",
    "    (h) Run the following cell to import the LinearRegression class.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "\n",
    "    The scikit-learn API makes it easy to train and evaluate models. All scikit-learn model classes have the following two methods:\n",
    "\n",
    "            fit: Train the model on the dataset given as input.\n",
    "\n",
    "            predict: Make a prediction from a set of explanatory variables given as input.\n",
    "\n",
    "    Below is an example of training a model with scikit-learn:\n",
    "\n",
    "    # Instantiation of the model\n",
    "    linreg = LinearRegression()\n",
    "\n",
    "    # Training the model on the training set\n",
    "    linreg.fit(X_train, y_train)\n",
    "\n",
    "    # Prediction of the target variable for the test dataset. These predictions are stored in y_pred.\n",
    "    y_pred = linreg.predict(X_test)\n",
    "\n",
    "    (i) Instantiate a LinearRegression model named lr.\n",
    "\n",
    "    (j) Train lr on the training dataset.\n",
    "\n",
    "    (k) Make a prediction on the training data. Store these predictions in y_pred_train.\n",
    "\n",
    "    (l) Make a prediction on the test data. Store these predictions in y_pred_test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "dict1 = {'y_pred_train' : y_pred_train,\n",
    "         'y_train_true' : y_train,\n",
    "         'y_train_difference' : (y_pred_train - y_train),\n",
    "         'difference_squared' : (y_pred_train - y_train)**2 }\n",
    "\n",
    "df_train = pd.DataFrame(dict1)\n",
    "df_train = df_train.astype(int)\n",
    "\n",
    "print(df_train)   \n",
    "\n",
    "print(y_pred_train.shape, y_pred_test.shape)\n",
    "\n",
    "print(\"MSE train : \",df_train.difference_squared.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0aaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Cozum\n",
    "# Instantiation of the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Training the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Prediction of the target variable for the TRAIN dataset\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "# Prediction of the target variable for the TEST dataset\n",
    "y_pred_test = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db8f2b6",
   "metadata": {},
   "source": [
    "## Evaluation of the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e786e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation of the model's performance\n",
    "\n",
    "    In order to evaluate the quality of the predictions of the model obtained thanks to the parameters ğ›½0\n",
    "\n",
    ", ..., ğ›½ğ‘—\n",
    "\n",
    ", there are several metrics already built in the scikit-learn library.\n",
    "\n",
    "One of the most used metrics for regression is the Mean Squared Error (MSE) which is defined under the name of mean_squared_error in the metrics submodule of scikit-learn.\n",
    "\n",
    "This function consists in calculating the average of the squared distances between the target variables and the predictions obtained thanks to the regression function.\n",
    "\n",
    "The following interactive figure shows how this error is calculated according to ğ›½1\n",
    "\n",
    ":\n",
    "\n",
    "        The blue dots represent the dataset for which we want to evaluate the quality of the predictions. Usually this is the test dataset.\n",
    "\n",
    "        The red line is the regression function configured by ğ›½1\n",
    "\n",
    ". In this example, ğ›½0\n",
    "\n",
    "    is set to 0 to simplify the illustration.\n",
    "\n",
    "    The green lines are the distances between the target variable and the predictions obtained thanks to the regression function parameterized by ğ›½1\n",
    "\n",
    "            .\n",
    "\n",
    "    The mean squared error is just the average of these squared distances.\n",
    "\n",
    "    (m) Run the next cell to display the interactive figure.\n",
    "\n",
    "    (n) Using the cursor below the figure, try to find a value of ğ›½1\n",
    "\n",
    "that minimizes the Mean Squared Error. Is this value unique?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9f58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from widgets import interactive_MSE\n",
    "\n",
    "interactive_MSE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c00839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    The mean_squared_error function of scikit-learn is used as follows:\n",
    "\n",
    "        mean_squared_error(y_true, y_pred)\n",
    "\n",
    "    where:\n",
    "\n",
    "            y_true contains the true values of the target variable.\n",
    "            y_pred contains the values predicted by our model for the same explanatory variables.\n",
    "\n",
    "    (o) Import the mean_squared_error function from the sklearn.metrics submodule.\n",
    "\n",
    "    (p) Evaluate the prediction quality of the model on training data. Store the result in a variable named mse_train.\n",
    "\n",
    "    (q) Evaluate model prediction quality on test data. Store the result in a variable named mse_test.\n",
    "\n",
    "    (r) Why is the MSE higher on the test dataset?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349eee84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(mse_train, 'MSE train')\n",
    "print(mse_test, 'MSE test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91311e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Cozum\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculation of the MSE between the target variable and the predictions made on the training dataset\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "# Calculation of the MSE between the target variable and the predictions made on the test dataset\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"MSE train lr:\", mse_train)\n",
    "print(\"MSE test lr:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca643b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    The mean squared error you will find should be around millions on the test data, which can be difficult to interpret.\n",
    "\n",
    "    This is why we are going to use another metric, the Mean Absolute Error which is at the same scale as the target variable.\n",
    "\n",
    "    (s) Import the mean_absolute_error function from the sklearn.metrics submodule.\n",
    "\n",
    "    (t) Evaluate the prediction quality on test and training data using the mean absolute error.\n",
    "\n",
    "    (u) From the DataFrame df, calculate the average purchase price on all vehicles. Do the model's predictions seem reliable to you?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a1665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(mae_train, 'MAE train')\n",
    "print(mae_test, 'MAE test')\n",
    "\n",
    "print(\"Avg Price : \", df['price'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92239478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculation of the MAE between the target variable and the predictions made on the training dataset\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "# Calculation of the MAE between the target variable and the predictions made on the test dataset\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"MAE train lr:\", mae_train)\n",
    "print(\"MAE test lr:\", mae_test)\n",
    "\n",
    "mean_price = df['price'].mean()\n",
    "\n",
    "print(\"\\nRelative error\", mae_test / mean_price)\n",
    "\n",
    "# The mean absolute error is around 20% of the average price, which is not optimal\n",
    "# but is still a good baseline for testing more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753c1e8a",
   "metadata": {},
   "source": [
    "## 2. Overfitting the data with another regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52563",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Overfitting the data with another regression model\n",
    "\n",
    "    We have just seen that with the LinearRegression class ofscikit-learn, the model was able to learn on the training data and generalize on the test data with an error rate of 20% on average.\n",
    "\n",
    "    In what follows we will create another regression model that learns very well on training data but generalizes very poorly on test data: this is called overfitting.\n",
    "\n",
    "    For this we will use a Machine Learning model called Gradient Boosting Regressor known for its tendancy to overfit.\n",
    "\n",
    "    (a) Run the following cell to import the GradientBoostingRegressor class contained in the ensemble submodule of scikit-learn and instantiate a GradientBoostingRegressor model named gbr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# These parameters have been chosen to overfit on purpose\n",
    "# Do not use them in practice\n",
    "gbr = GradientBoostingRegressor(n_estimators = 1000,\n",
    "                                max_depth = 10000,\n",
    "                                max_features = 15,\n",
    "                                validation_fraction = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17711810",
   "metadata": {},
   "outputs": [],
   "source": [
    "    (b) Train the model gbr using its fit method.\n",
    "\n",
    "    (c) Make predictions on the test and training datasets. Store these predictions in y_pred_test_gbr andy_pred_train_gbr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccd39bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train_gbr = gbr.predict(X_train)\n",
    "\n",
    "y_pred_test_gbr = gbr.predict(X_test)\n",
    "\n",
    "\n",
    "# Original Cozum\n",
    "# Training the model on the training dataset\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Prediction of the target variable for the TRAIN dataset\n",
    "y_pred_train_gbr = gbr.predict(X_train)\n",
    "\n",
    "# Prediction of the target variable for the TEST dataset\n",
    "y_pred_test_gbr = gbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ef2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    After instantiating our model, training it on the training data and making the predictions, we must then evaluate its performance.\n",
    "\n",
    "    (d) Calculate the MSE on the training data and the test data using the mean_squared_error function then display the results.\n",
    "\n",
    "    (e) Calculate the MAE for the training data and the test data using the mean_absolute_error function then display the results .\n",
    "\n",
    "    (f) After having calculated the average of the price column, calculate the relative error of the model on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fee963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "mse_train = mean_squared_error(y_train, y_pred_train_gbr)\n",
    "\n",
    "mse_test = mean_squared_error(y_test, y_pred_test_gbr)\n",
    "\n",
    "print(mse_train, \"MSE train\")\n",
    "print(mse_test, \"MSE test\\n\")\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train_gbr)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test_gbr)\n",
    "\n",
    "print(mae_train, \"MAE train\")\n",
    "print(mae_test, \"MAE test\\n\")\n",
    "\n",
    "mean_price_gbr = df.price.mean()\n",
    "print(\"Relative Error : \", mae_test / mean_price_gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec64dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MSE\n",
    "\n",
    "# Calculation of the MSE between the target variable and the predictions made on the training dataset\n",
    "mse_train_gbr = mean_squared_error(y_train, y_pred_train_gbr)\n",
    "\n",
    "# Calculation of the MSE between the target variable and the predictions made on the test dataset\n",
    "mse_test_gbr = mean_squared_error(y_test, y_pred_test_gbr)\n",
    "\n",
    "print(\"MSE train gbr:\", mse_train_gbr)\n",
    "print(\"MSE test gbr:\", mse_test_gbr, \"\\n\")\n",
    "\n",
    "\n",
    "### MAE\n",
    "\n",
    "# Calculation of the MAE between the target variable and the predictions made on the training dataset\n",
    "mae_train_gbr = mean_absolute_error(y_train, y_pred_train_gbr)\n",
    "\n",
    "# Calculation of the MAE between the target variable and the predictions made on the test dataset\n",
    "mae_test_gbr = mean_absolute_error(y_test, y_pred_test_gbr)\n",
    "\n",
    "print(\"MAE train gbr:\", mae_train_gbr)\n",
    "print(\"MAE test gbr:\", mae_test_gbr, \"\\n\")\n",
    "\n",
    "mean_price_gbr = df ['price'].mean()\n",
    "\n",
    "print(\"Relative error\", mae_test_gbr / mean_price_gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083742dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    Here is an example of results that we could obtain with these two models.\n",
    "\n",
    "    For the linear regression with LinearRegression we had:\n",
    "\n",
    "            MAE train lr = 1588.131591267774\n",
    "            MAE test lr = 2105.5002712214014\n",
    "\n",
    "    For the regression with GradientBoostingRegressor we have:\n",
    "\n",
    "            MAE train gbr: 27.533333333339847\n",
    "            MAE test gbr: 1393.013371545563\n",
    "\n",
    "    The mean absolute error obtained on the training set by the GradientBoostingRegressor model is only 27.5 against 1588 for the linear regression. The GradientBoostingRegressor model is very powerful and is able to learn the training data almost \"by heart\" which explains this difference in performance.\n",
    "\n",
    "    It is for this reason that the performance of the model should be evaluated on the test dataset. Indeed, the average absolute error of the GradientBoostingRegressor model is 1393, which is very far from the performance obtained on the training data.\n",
    "\n",
    "    This is an example of blatant overfitting. Even if the performance of the GradientBoostingRegressor is superior to that of the linear regression on the test data, you should always be wary of too high a performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc734d",
   "metadata": {},
   "source": [
    "## 3. Going further: Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7974d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Going further: Polynomial Regression\n",
    "\n",
    "    In many cases, the relationship between the variables ğ‘¥\n",
    "\n",
    "and ğ‘¦ is not linear. This does not allow us to use linear regression to predict ğ‘¦\n",
    "\n",
    ". We could then propose a quadratic model such as:\n",
    "\n",
    "ğ‘¦=ğ›½0+ğ›½1ğ‘¥+ğ›½2ğ‘¥2\n",
    "\n",
    "    (a) Run the next cell to display the interactive figure.\n",
    "\n",
    "    (b) Find the optimal parameters for ğ›½0, ğ›½1 and ğ›½2 that best approximate the data on the scatter plot.\n",
    "\n",
    "    (c) Set ğ›½2 to 0 and vary ğ›½0 and ğ›½1. Which model do you recognize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191a215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from widgets import polynomial_regression\n",
    "\n",
    "polynomial_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Polynomial regression is equivalent to performing a classical linear regression from polynomial functions of the explanatory variable of arbitrary degree. Polynomial regression is much more flexible than classical linear regression because it can approach any type of continuous function.\n",
    "\n",
    "When we have several explanatory variables, the polynomial variables can also be calculated by products between the explanatory variables. For example, if we have three variables, then the second-order polynomial regression model becomes:\n",
    "\n",
    "ğ‘¦â‰ˆğ›½0+ğ›½1ğ‘¥21+ğ›½2ğ‘¥22+ğ›½3ğ‘¥23+ğ›½4ğ‘¥1ğ‘¥2+ğ›½5ğ‘¥2ğ‘¥3+ğ›½6ğ‘¥1ğ‘¥3\n",
    "\n",
    "    If we had more explanatory variables or wanted to increase the degree of polynimial regression, the number of explanatory variables would explode, which could induce overfitting.\n",
    "\n",
    "    (d) Run the next cell to display the interactive figure.\n",
    "\n",
    "    The scatter plot was generated with the same trend as the previous figure. The red line corresponds to the optimal polynomial regression function obtained on these data.\n",
    "\n",
    "    (e) Taking into account the scatter plot in the previous figure, find the degree of the polynomial regression that best captures the trend of the data.\n",
    "\n",
    "    (f) Set d to 20. Do you think this regression function would give good predictions on the scatter plot in the previous figure?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f92af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from widgets import polynomial_regression2\n",
    "\n",
    "polynomial_regression2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbb83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    To train a Polynomial regression model with scikit-learn, we must first calculate the polynomial variables from the data. This can be done using the PolynomialFeatures class of the preprocessing submodule:\n",
    "\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    poly_feature_extractor = PolynomialFeatures(degree = 2)\n",
    "\n",
    "        The degree parameter defines the degree of the polynomial features to be calculated.\n",
    "\n",
    "    The poly_feature_extractor object is not a prediction model. This type of object is called a Transformer and it can be used with the following two methods:\n",
    "\n",
    "            fit: does nothing in this case. This method is generally used to calculate the parameters necessary to apply a transformation to the data.\n",
    "\n",
    "            transform: Applies the transformation to the dataset. In this case, the method returns the polynomial features of the dataset.\n",
    "\n",
    "    These two methods can be called simultaneously using the fit_transform method. We can compute the polynomial features on X_train andX_test as follows:\n",
    "\n",
    "    X_train_poly = poly_feature_extractor.fit_transform(X_train)\n",
    "\n",
    "    X_test_poly = poly_feature_extractor.transform(X_test)\n",
    "\n",
    "    (g) Import the PolynomialFeatures class from the preprocessing submodule of sklearn.\n",
    "\n",
    "    (h) Instantiate an object of class PolynomialFeatures with the argument degree = 3 and name it poly_feature_extractor.\n",
    "\n",
    "    (i) Apply the transformation of poly_feature_extractor on X_train and X_test and store the results in X_train_poly and X_test_poly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594190fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_feature_extractor = PolynomialFeatures(degree = 3)\n",
    "\n",
    "X_train_poly = poly_feature_extractor.fit_transform(X_train)\n",
    "\n",
    "X_test_poly = poly_feature_extractor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f599d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Cozum\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_feature_extractor = PolynomialFeatures(degree = 3)\n",
    "\n",
    "# Applying the transformation on X_train et X_test\n",
    "X_train_poly = poly_feature_extractor.fit_transform(X_train)\n",
    "X_test_poly = poly_feature_extractor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1701eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    (j) Train a linear regression model on the data (X_train_poly, y_train).\n",
    "\n",
    "    (k) Evaluate its performance on training data and test data (X_test_poly, y_test). Are we in an overfitting regime?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a863de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train_poly, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train_poly)\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "print(\"MAE train : \",mae_train)\n",
    "\n",
    "y_pred_test = lr.predict(X_test_poly)\n",
    "\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"MAE test : \", mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Cozum\n",
    "# Instantiation of a linear regression model\n",
    "polyreg = LinearRegression()\n",
    "\n",
    "# Training of the model on polynomial features\n",
    "polyreg.fit(X_train_poly, y_train)\n",
    "\n",
    "# Evaluation of the model on the training data\n",
    "y_pred_train = polyreg.predict(X_train_poly)\n",
    "print(\"MAE Train:\", mean_absolute_error(y_train, y_pred_train), '\\n')\n",
    "\n",
    "\n",
    "# Evaluation of the model on the test data\n",
    "y_pred_test = polyreg.predict(X_test_poly)\n",
    "print(\"MAE Test:\", mean_absolute_error(y_test, y_pred_test), '\\n')\n",
    "\n",
    "\n",
    "print(\"We are absolutely in an overfitting regime.\")\n",
    "print(\"The polynomial regression model performs well on training data but not on test data.\")\n",
    "print(\"The third-order polynomial regression model performs significantly worse than a simple linear regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4793d8",
   "metadata": {},
   "source": [
    "## Conclusion and recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4c25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion and recap\n",
    "\n",
    "    In this course, you have been introduced to solving a regression problem with machine learning.\n",
    "\n",
    "    We used the scikit-learn library to instantiate regression models like LinearRegression or GradientBoostingRegressor and also apply transformations on the data like extracting polynomial features.\n",
    "\n",
    "    The different steps that we have studied are the basis of any solution to a Machine Learning problem:\n",
    "\n",
    "            The data is prepared by separating the explanatory variables from the target variable.\n",
    "\n",
    "            We split the dataset into two sets (a training set and a test set) using the train_test_split function of thesklearn.model_selection submodule.\n",
    "\n",
    "            We instantiate a model like LinearRegression or GradientBoostingRegressor thanks to the class' constructor.\n",
    "\n",
    "            We train the model on the training dataset using the fit method.\n",
    "\n",
    "            We perform a prediction on the test dataset using the predict method.\n",
    "\n",
    "            We evaluate the performance of our model by calculating the error between these predictions and the true values of the target variable from the test data.\n",
    "\n",
    "    The performance evaluation for a regression model is easily done using the mean_squared_error or mean_absolute_error functions of the metrics submodule of sklearn.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74d3b069",
   "metadata": {},
   "source": [
    "## RECAP Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc33b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECAP Linear Regression \n",
    "# Load Data\n",
    "df = pd.read_csv('automobiles.csv', index_col = 0)\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# Explanatory variables\n",
    "X = df.drop(['target'], axis = 1)\n",
    "\n",
    "# Target variable\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "\n",
    "# Splitting the dataset into a training set (85%) and a test set (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "\n",
    "\n",
    "# Instantiation of the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Training the model\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Prediction of the target variable for the TRAIN dataset\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "# Prediction of the target variable for the TEST dataset\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# Calculation of the MSE between the target variable and the predictions made on the training dataset\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "\n",
    "# Calculation of the MSE between the target variable and the predictions made on the test dataset\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"MSE train lr:\", mse_train)\n",
    "print(\"MSE test lr:\", mse_test)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Calculation of the MAE between the target variable and the predictions made on the training dataset\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "\n",
    "# Calculation of the MAE between the target variable and the predictions made on the test dataset\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(\"MAE train lr:\", mae_train)\n",
    "print(\"MAE test lr:\", mae_test)\n",
    "\n",
    "mean_price = df['price'].mean()\n",
    "\n",
    "print(\"\\nRelative error\", mae_test / mean_price)\n",
    "\n",
    "# The mean absolute error is around 20% of the average price, which is not optimal\n",
    "# but is still a good baseline for testing more advanced models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b514cde4-16cf-4c4b-8d48-6bbabfe5e1ea",
   "metadata": {},
   "source": [
    "Accuracy (DoÄŸruluk): Bir sÄ±nÄ±flandÄ±rma algoritmasÄ± tarafÄ±ndan doÄŸru olarak tahmin edilen Ã¶rneklerin oranÄ±dÄ±r. \n",
    "Yani doÄŸru tahmin edilen Ã¶rneklerin toplam Ã¶rneklere oranÄ±dÄ±r. \n",
    "Ã–rnek olarak, bir hastalÄ±k teÅŸhisi yapmak iÃ§in bir sÄ±nÄ±flandÄ±rma modeli kullanÄ±rsak,\n",
    "doÄŸru olarak teÅŸhis edilen hastalarÄ±n sayÄ±sÄ±nÄ±n toplam hastalara oranÄ± accuracy olarak ifade edilir.\n",
    "\n",
    "Recall (DuyarlÄ±lÄ±k): GerÃ§ek pozitif Ã¶rneklerin tamamÄ±nÄ±n doÄŸru olarak tahmin edilme oranÄ±dÄ±r. \n",
    "Yani gerÃ§ekte hastalÄ±ÄŸÄ± olan hastalarÄ±n, hastalÄ±ÄŸÄ± olanlarÄ±n tÃ¼mÃ¼ olarak tanÄ±mlanma oranÄ±dÄ±r. \n",
    "Ã–rnek olarak, kanser taramasÄ± yapmak iÃ§in bir sÄ±nÄ±flandÄ±rma modeli kullanÄ±rsak, \n",
    "gerÃ§ekte kanser hastasÄ± olanlarÄ±n tamamÄ±nÄ±n doÄŸru ÅŸekilde tespit edilme oranÄ± recall olarak ifade edilir.\n",
    "\n",
    "Precision (Kesinlik): Bir sÄ±nÄ±flandÄ±rma algoritmasÄ±nÄ±n doÄŸru olarak tahmin ettiÄŸi pozitif Ã¶rneklerin oranÄ±dÄ±r. \n",
    "Yani, modelin doÄŸru olarak tahmin ettiÄŸi pozitif Ã¶rneklerin gerÃ§ekte pozitif olanlarÄ±n tÃ¼mÃ¼ne oranÄ±dÄ±r. \n",
    "Ã–rnek olarak, bir spam filtresi oluÅŸturmak iÃ§in bir sÄ±nÄ±flandÄ±rma modeli kullanÄ±rsak, \n",
    "gerÃ§ekten spam olanlarÄ±n tahmini doÄŸru ÅŸekilde spam olarak sÄ±nÄ±flandÄ±rÄ±ldÄ±ÄŸÄ± oran precision olarak ifade edilir.m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df3f63e-23a1-460b-8a9c-e5fcb2781914",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc764de-bcf8-4f50-bc8f-fbcca02b3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part II: Simple classification models\n",
    "\n",
    "    For this second part of an introduction to the scikit-learn module, we will focus on the second type of problem in Machine Learning: the classification problem.\n",
    "\n",
    "    The objective of this introduction is to:\n",
    "\n",
    "            Introduce the classification problem.\n",
    "            Learn to use the scikit-learn module to build a classification model, also known as a \"classifier\".\n",
    "            Introduce metrics adapted to the evaluation of classification models.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76f4b58b",
   "metadata": {},
   "source": [
    "## Introduction to classifcation\n",
    "\n",
    "Objective of classification\n",
    "\n",
    "    In supervised learning, the objective is to predict the value of a target variable from explanatory variables.\n",
    "\n",
    "            In a regression problem, the target variable takes continuous values. These values are numerical: price of a house, quantity of oxygen in the air of a city, etc. The target variable can therefore take an infinity of values.\n",
    "\n",
    "            In a classification problem, the target variable takes discrete values. These values can be numeric or literal, but in both cases the target variable takes a finite number of values.\n",
    "\n",
    "    The different values taken by the target variable are called classes.\n",
    "\n",
    "    The objective of classification therefore consists in predicting the class of an observation from its explanatory variables.\n",
    "\n",
    "An example of classification\n",
    "\n",
    "    We will look at a problem of a binary classification, i.e. where there are two classes. We are trying to determine whether the water in a stream is drinkable or not depending on its concentration of toxic substances and its mineral salts content.\n",
    "\n",
    "    The two classes are therefore 'drinkable' and 'non-drinkable'.\n",
    "\n",
    "<img src=\"Photos\\sklearn_intro_classification_binaire_en.png\" width=\"400\" height=\"400\">\n",
    "\n",
    "    In the figure above, each point represents a stream whose position on the map is defined by its values for the concentration of toxic substances and the content of mineral salts.\n",
    "\n",
    "    The objective will be to build a model capable of assigning one of the two classes ('drinkable' / 'non-drinkable') to a stream of which only these two variables are known.\n",
    "\n",
    "    The figure above suggests the existence of two zones allowing easy classification of streams:\n",
    "\n",
    "            An area where the streams are drinkable (top left).\n",
    "\n",
    "            An area where the streams are not drinkable (bottom right).\n",
    "\n",
    "    We would like to create a model capable of separating the dataset into two parts corresponding to these areas.\n",
    "\n",
    "    A simple technique would be to separate the two areas using a line.\n",
    "\n",
    "    (a) Run the next cell to display the interactive figure.\n",
    "\n",
    "        The orange dots are the drinkable streams and the blue dots are the non-drinkable streams.\n",
    "\n",
    "        The red arrow corresponds to a vector defined by ğ‘¤=(ğ‘¤1,ğ‘¤2)\n",
    "\n",
    ". The red line corresponds to the orthogonal (i.e. perpendicular) plane to ğ‘¤. You can change the coordinates of the vector ğ‘¤\n",
    "\n",
    "        in two ways:\n",
    "\n",
    "                By moving the sliders of w_1 andw_2.\n",
    "                By clicking on the values to the right of the sliders and typing the desired value.\n",
    "\n",
    "    (b) Try to find a vector ğ‘¤\n",
    "\n",
    "such that the plane orthogonal to ğ‘¤\n",
    "\n",
    "    perfectly separates the two stream classes.\n",
    "\n",
    "    (c) A possible solution is given by the vector ğ‘¤=(âˆ’1.47,0.84)\n",
    "\n",
    ". Does the vector ğ‘¤=(1.47,âˆ’0.84) also give a solution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320bcc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classification_widgets import linear_classification\n",
    "\n",
    "linear_classification()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86d8863d",
   "metadata": {},
   "source": [
    "##  \n",
    "The classification we just performed is of linear type, that is to say that we used a flat linear plane to separate our classes.\n",
    "\n",
    "This plane was parametrized by the vector ğ‘¤\n",
    ". Thus, the objective of linear classification models is to find the vector ğ‘¤\n",
    "\n",
    "allowing the best possible separation of the different classes. Each model of linear type has its own technique to find this vector.\n",
    "\n",
    "There are also non-linear classification models, which we will see later.\n",
    "\n",
    "<img src=\"Photos\\sklearn_intro_classification_lin_non_lin_en.png\" width=\"900\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71efb3bd",
   "metadata": {},
   "source": [
    "## 1. Using scikit-learn for classification\n",
    "\n",
    "    We will now introduce the main tools of the scikit-learn module for solving a classification problem.\n",
    "\n",
    "    In this exercise we will use the Congressional Voting Records dataset, containing a number of votes cast by members of Congress of the United States House of Representatives.\n",
    "\n",
    "    The objective of our classification problem will be to predict the political party (\"Democrat\" or \"Republican\") of the members of the House of Representatives according to their votes on subjects like the education, health, budget, etc.\n",
    "\n",
    "    The explanatory variables will therefore be the votes on various subjects and the target variable will be the \"democrat\" or \"republican\" political party.\n",
    "\n",
    "    To solve this problem we will use:\n",
    "\n",
    "            A non-linear classification model: K-Nearest Neighbors.\n",
    "\n",
    "            A linear classification model: Logistic Regression.\n",
    "\n",
    "Data preparation\n",
    "\n",
    "    (a) Run the following cell to import the pandas and numpy modules needed for the exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "565cbd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(b) Load the data contained in the file 'votes.csv' into a DataFrame named votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b3a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "votes = pd.read_csv('votes.csv')\n",
    "votes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b53e68c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>party</th>\n",
       "      <th>infants</th>\n",
       "      <th>water</th>\n",
       "      <th>budget</th>\n",
       "      <th>physician</th>\n",
       "      <th>salvador</th>\n",
       "      <th>religious</th>\n",
       "      <th>satellite</th>\n",
       "      <th>aid</th>\n",
       "      <th>missile</th>\n",
       "      <th>immigration</th>\n",
       "      <th>synfuels</th>\n",
       "      <th>education</th>\n",
       "      <th>superfund</th>\n",
       "      <th>crime</th>\n",
       "      <th>duty_free_exports</th>\n",
       "      <th>eaa_rsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>republican</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>democrat</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "      <td>n</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        party infants water budget physician salvador religious satellite aid  \\\n",
       "0  republican       n     y      n         y        y         y         n   n   \n",
       "1  republican       n     y      n         y        y         y         n   n   \n",
       "2    democrat       n     y      y         n        y         y         n   n   \n",
       "\n",
       "  missile immigration synfuels education superfund crime duty_free_exports  \\\n",
       "0       n           y        n         y         y     y                 n   \n",
       "1       n           n        n         y         y     y                 n   \n",
       "2       n           n        y         n         y     y                 n   \n",
       "\n",
       "  eaa_rsa  \n",
       "0       y  \n",
       "1       n  \n",
       "2       n  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "votes = pd.read_clipboard()\n",
    "votes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c97d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "In order to briefly visualize our data:\n",
    "\n",
    "    (c) Display the number of rows and columns of votes.\n",
    "\n",
    "    (d) Show a preview of the first 20 rows of votes.\n",
    "\n",
    "# Shape of the DataFrame\n",
    "print('The DataFrame has', votes.shape[0], 'rows and', votes.shape[1], 'columns.')\n",
    "\n",
    "# Display the first 20 rows\n",
    "votes.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d670092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        The first column \"party\" contains the name of the political party to which each member of the Congress of the House of Representatives belongs. This is the target variable.\n",
    "\n",
    "        The following 16 columns contain the votes of each member of Congress on legislative proposals:\n",
    "\n",
    "            'y' indicates that the elected member voted for the bill.\n",
    "\n",
    "            'n' indicates that the elected member voted against the bill.\n",
    "\n",
    "    In order to use the data in a classification model, it is necessary to transform these columns into binary numeric values, i.e. either 0 or 1.\n",
    "\n",
    "    (e) For each of the columns 1 to 16 (column 0 being our target variable), replace the values 'y' by 1 and 'n' by 0. To do so, we can use the replace method from the DataFrame class.\n",
    "\n",
    "    (f) Display the first 10 rows of the modified DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5e7b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the values\n",
    "votes = votes.replace(('y', 'n'), (1, 0))\n",
    "\n",
    "# Display the first 10 rows of the DataFrame\n",
    "votes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791850e",
   "metadata": {},
   "source": [
    "## Separation of the variables\n",
    "    (g) In a DataFrame named X, store the explanatory variables of the dataset (all columns except 'party'). For this, you can use the drop method of a DataFrame.\n",
    "\n",
    "    (h) In a DataFrame named y, store the target variable ('party').\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302db71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separation of the variables\n",
    "\n",
    "X = votes.drop(['party'], axis = 1)\n",
    "y = votes['party']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb67145",
   "metadata": {},
   "source": [
    "\n",
    "## train_test_split\n",
    "    As for the regression problem, we will have to split the data set into 2 sets: a training set and a test set. As a reminder:\n",
    "\n",
    "            The training set is used to train the classification model, that is to say find the parameters of the model which best separate the classes.\n",
    "\n",
    "            The test set is used to evaluate the model on data that it has never seen. This evaluation will allow us to judge the generalizability of the model.\n",
    "\n",
    "    (i) Import the train_test_split function from the sklearn.model_selection submodule. Remember that this function is used as follows:\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "    (j) Split the data into a training set (X_train, y_train) and a test set (X_test, y_test) keeping 20% of the data for the test set.\n",
    "\n",
    "    To eliminate the randomness of the train _test_split function, you can use the random_state parameter with an integer value (for example random_state = 2). This will make it so every time you use the function with the argument random_state = 2, the datasets produced will be the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47fd735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b2297",
   "metadata": {},
   "source": [
    "## Non-linear classification: K-Nearest Neighbors model\n",
    "\n",
    "    In order to assign a class to an observation, the K-Nearest Neighbors algorithm considers, as its name suggests, the K nearest neighbors of the observation and determines the most represented class among these neighbors.\n",
    "\n",
    "    Concretely, the algorithm is as follows:\n",
    "\n",
    "            Suppose that K = 5.\n",
    "\n",
    "            For an observation that we want to classify, we will look at the 5 points of the training set that are closest to our observation. The distance metric used is often the euclidian norm.\n",
    "\n",
    "            If among the 5 neighbors, the majority is \"democrat\", then the observation will be classified \"democrat\".\n",
    "\n",
    "    To train a K-Nearest Neighbors model for our problem, we'll use the KNeighborsClassifer class from the neighbors submodule of scikit-learn.\n",
    "\n",
    "    The number K of neighbors to consider is entered using the parameter n_neighbors of the KNeighborsClassifer constructor.\n",
    "\n",
    "    (k) Run the following cell to import the KNeighborsClassifer class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a73ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c47bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    (l) Instantiate a KNeighborsClassifier model named knn which will consider the 6 nearest neighbors for classification.\n",
    "\n",
    "    (m) Using the fit method, train the model knn on the training dataset.\n",
    "\n",
    "    (n) Using the predict method, perform a prediction on the test dataset. Store these predictions in y_pred_test_knn and display the first 10 predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39170854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test_knn = knn.predict(X_test)\n",
    "\n",
    "y_pred_test_knn[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fadf037a",
   "metadata": {},
   "source": [
    "## Linear Classification: Logistic Regression\n",
    "\n",
    "    The logistic regression model is closely related to the linear regression model seen in the previous notebook.\n",
    "\n",
    "    They should not be confused since they do not solve the same types of problems:\n",
    "\n",
    "            Logistic Regression is used for classification (predict classes).\n",
    "\n",
    "            Linear regression is used for regression (predict a quantitative variable).\n",
    "\n",
    "    The linear regression model was defined with the following formula:\n",
    "\n",
    "    ğ‘¦â‰ˆğ›½0+âˆ‘ğ‘—=1ğ‘ğ›½ğ‘—ğ‘¥ğ‘—\n",
    "\n",
    "Logistic regression no longer estimates ğ‘¦\n",
    "directly but the probability that ğ‘¦\n",
    "\n",
    "is equal to 0 or 1. Thus, the model is defined by the formula:\n",
    "\n",
    "ğ‘ƒ(ğ‘¦=1)=ğ‘“(ğ›½0+âˆ‘ğ‘—=1ğ‘ğ›½ğ‘—ğ‘¥ğ‘—)\n",
    "\n",
    "Where\n",
    "ğ‘“(ğ‘¥)=1 / (1+ğ‘’^âˆ’ğ‘¥)\n",
    "\n",
    "The ğ‘“\n",
    "function, often called sigmoid or logistic function, transforms the linear combination ğ›½0+âˆ‘ğ‘ğ‘—=1ğ›½ğ‘—ğ‘¥ğ‘—\n",
    "\n",
    "into a value between 0 and 1 that can be interpreted as a probability:\n",
    "\n",
    "        If ğ›½0+âˆ‘ğ‘ğ‘—=1ğ›½ğ‘—ğ‘¥ğ‘—\n",
    "\n",
    "is positive, then ğ‘ƒ(ğ‘¦=1)>0.5\n",
    "\n",
    "    , so the predicted class of the observation will be 1.\n",
    "\n",
    "    If ğ›½0+âˆ‘ğ‘ğ‘—=1ğ›½ğ‘—ğ‘¥ğ‘—\n",
    "\n",
    "is negative, then ğ‘ƒ(ğ‘¦=1)<0.5, i.e. ğ‘ƒ(ğ‘¦=0)>0.5\n",
    "\n",
    "            , so the predicted class of the observation will be 0.\n",
    "\n",
    "    (o) Import the LogisticRegression class from the linear_model submodule of scikit-learn.\n",
    "\n",
    "    (p) Instantiate a LogisticRegression model named logreg without specifying constructor arguments.\n",
    "\n",
    "    (q) Train the model on the training dataset.\n",
    "\n",
    "    (r) Make a prediction on the test dataset. Store these predictions in y_pred_test_logreg and display the first 10 predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24809a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test_logreg = logreg.predict(X_test)\n",
    "\n",
    "y_pred_test_logreg[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8adf712e",
   "metadata": {},
   "source": [
    "## 2. Evaluate the performance of a classification model\n",
    "\n",
    "    There are different metrics to evaluate the performance of classification models such as:\n",
    "\n",
    "            The accuracy.\n",
    "\n",
    "            The precision and the recall.\n",
    "\n",
    "            The F1-score.\n",
    "\n",
    "    Each metric assesses the performance of the model with a different approach.\n",
    "\n",
    "    In order to explain these concepts, we will introduce 4 very important terms.\n",
    "\n",
    "    Arbitrarily, we will choose that the class 'republican' will be the positive class (1) and 'democrat' will be the negative class (0).\n",
    "\n",
    "    Thus, we will call:\n",
    "\n",
    "            True Positive (TP) an observation classified as positive ('republican') by the model which is indeed positive ('republican').\n",
    "\n",
    "            False Positive (FP) an observation classified as positive ('republican') by the model which was actually negative ('democrat').\n",
    "\n",
    "            True Negative (TN) an observation classified as negative ('democrat') by the model and which is indeed negative('democrat').\n",
    "\n",
    "            False Negative (FN) an observation classified as negative ('democrat') by the model which was actually positive ('republican').\n",
    "\n",
    "<img src=\"Photos\\sklearn_intro_positif_negatif_en.png\" width=\"700\" height=\"400\">\n",
    "\n",
    "The accuracy is the most common metric used to evaluate a model. It simply corresponds to the rate of correct predictions made by the model.\n",
    "\n",
    "We suppose that we have ğ‘›\n",
    "observations. We denote by TP the number of True Positives and TN\n",
    "\n",
    "the number of True Negatives. Then the accuracy is given by:\n",
    "\n",
    "**accuracy= TP+TN / ğ‘›**\n",
    "\n",
    "The precision is a metric which answers the question: Among all the positive predictions of the model, how many are true positives? If we denote by FP\n",
    "\n",
    "the number of False Positives of the model, then the precision is given by:\n",
    "\n",
    "**precision= TP /TP+FP**\n",
    "\n",
    "A high precision score tells us the model does not blindly classify everyone as positive.\n",
    "\n",
    "The recall is a metric that quantifies the proportion of truly positive observations that were correctly classified as positive by the model.\n",
    "\n",
    "If we write FN\n",
    "\n",
    "as the number of False Negatives, then the callback is given by:\n",
    "\n",
    "**recall= TP / TP+FN**\n",
    "\n",
    "A high recall score tells us the model is able to properly detect the truly positive observations.\n",
    "\n",
    "The confusion matrix counts the values of TP, TN, FP and FN for a set of predictions, which allows us to calculate the three previous metrics:\n",
    "\n",
    "**ConfusionMatrix =  [TN FP**\n",
    "                   \n",
    "                     FN TP]\n",
    "\n",
    "    The confusion_matrix function of the sklearn.metrics submodule generates the confusion matrix from the predictions of a model:\n",
    "\n",
    "    confusion_matrix (y_true, y_pred)\n",
    "\n",
    "    As a reminder:\n",
    "\n",
    "            y_true contains the true values of y.\n",
    "\n",
    "            y_pred contains the values of y predicted by the model.\n",
    "\n",
    "    (a) Import the confusion_matrix function from sklearn.metrics.\n",
    "\n",
    "    (b) Calculate the confusion matrix of the predictions produced by the model knn. These predictions were stored in y_pred_test_knn.\n",
    "\n",
    "    (c) Display the confusion matrix. How many false positives have occurred? The positive class corresponds to 'republican'.\n",
    "\n",
    "    (d) Using the formulas given above, calculate the accuracy, precision, and recall scores of the knn model on the test set. You can use tuple assignment to deconstruct the confusion matrix:\n",
    "\n",
    "**(TN, FP), (FN, TP) = confusion_matrix(y_true, y_pred)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b972f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here\n",
    "\n",
    "# NOTE\n",
    "# 'republican' = 1\n",
    "# 'democrat' = 0 \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "y_pred_test_knn = knn.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test_knn)\n",
    "print('confusion_matrix : \\n', cm)\n",
    "print(cm[0])\n",
    "print(cm[0,0])\n",
    "\n",
    "(TN, FP), (FN, TP) = confusion_matrix(y_test, y_pred_test_knn)\n",
    "accuracy = (TP + TN) / len(y_test)\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "\n",
    "print('Accuracy :', accuracy, '\\nPrecision :', precision, '\\nRecall :', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7704052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Computation and display of the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test_knn)\n",
    "print(\"Confusion Matrix:\\n\",  conf_matrix)\n",
    "\n",
    "print(\"\\nThe knn model made\", conf_matrix[0,1], \"False Positives.\")\n",
    "\n",
    "# Computation of the accuracy, precision and recall\n",
    "(TN, FP), (FN, TP) = confusion_matrix(y_test, y_pred_test_knn)\n",
    "n = len(y_test)\n",
    "\n",
    "print(\"\\nKNN Accuracy:\", (TP + TN) / n)\n",
    "\n",
    "print(\"\\nKNN Precision:\", TP / (TP + FP))\n",
    "\n",
    "print(\"\\nKNN Recall:\", TP / (TP + FN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397c4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    The display of the confusion matrix can also be done with the pd.crosstab function as we had done in a previous notebook:\n",
    "\n",
    "    pd.crosstab(y_test, y_pred_test_knn, rownames = ['Reality'], colnames = ['Prediction'])\n",
    "\n",
    "    Which in our case will produce the following DataFrame:\n",
    "\n",
    "    Prediction  democrat  republican\n",
    "\n",
    "    Reality \n",
    "    democrat \t48 \t5\n",
    "    republican \t2 \t32\n",
    "\n",
    "    For this dataset, the KNN model performs quite well. When the classes are balanced, i.e. there are about as many positives as there are negatives in the dataset, accuracy is a good enough metric to assess the performance.\n",
    "\n",
    "    However, as you will see later, when a class is dominant, precision and recall are much more relevant metrics.\n",
    "\n",
    "    If you think you cannot remember the formulas for the metrics of accuracy, precision and recall, do not worry! The sklearn.metrics submodule contains functions to calculate them quickly:\n",
    "\n",
    "    accuracy_score(y_test, y_pred_test_knn)\n",
    "    >>> 0.9195402298850575\n",
    "\n",
    "    (e) Import the accuracy_score, precision_score and recall_score functions from the sklearn.metrics submodule.\n",
    "\n",
    "    (f) Display the confusion matrix of the predictions made by the logreg model using pd.crosstab.\n",
    "\n",
    "    (g) Calculate the accuracy, precision and recall of model predictions logreg. To use the precision_score and recall_score metrics, you will need to fill in the argument pos_label = 'republican' in order to specify that the 'republican' class is the positive class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcba24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Computation and display of the confusion matrix\n",
    "pd.crosstab(y_test, y_pred_test_logreg, rownames=['Reality'], colnames=['Prediction'])\n",
    "\n",
    "# Computation of the accuracy, precision and recall\n",
    "print(\"\\nLogReg Accuracy:\", accuracy_score(y_test, y_pred_test_logreg))\n",
    "\n",
    "print(\"\\nLogReg Precision:\", precision_score(y_test, y_pred_test_logreg, pos_label = 'republican'))\n",
    "\n",
    "print(\"\\nLogReg Recall:\", recall_score(y_test, y_pred_test_logreg, pos_label = 'republican'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d302c68",
   "metadata": {},
   "source": [
    "## Classification Report\n",
    "\n",
    "    The classification_report function of the sklearn.metrics submodule displays all these metrics for each class.\n",
    "\n",
    "    (h) Import the classification_report function from the sklearn.metrics submodule.\n",
    "\n",
    "    (i) Display using the print and classification_report functions the classification reports of the models logreg and knn on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a2b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"LogReg report:\\n\", classification_report(y_test, y_pred_test_logreg))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"KNN report:\\n\", classification_report(y_test, y_pred_test_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9df953",
   "metadata": {},
   "source": [
    "## F1-Score\n",
    "\n",
    "    The classification report is a little more complete than what we have done so far. It contains an additional metric: the F1-Score.\n",
    "\n",
    "    The F1-Score is a sort of average between precision and recall. The F1-Score adapts very well to classification problems with balanced or unbalanced classes.\n",
    "\n",
    "    For most classification problems, the model with the highest F1-Score will be considered the model whose recall and precision performances are the most balanced, and is therefore preferable to others.\n",
    "\n",
    "    (j) Import the f1_score function from submodule sklearn.metrics.\n",
    "\n",
    "    (k) Compare the F1-Scores of the models knn and logreg on the test set. Which model has the best performance? As for the recall and the precision, it will be necessary to fill in the argument pos_label = 'republican'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1 KNN:\", f1_score(y_test, y_pred_test_knn, pos_label = 'republican'))\n",
    "\n",
    "print(\"F1 LogReg:\", f1_score(y_test, y_pred_test_logreg, pos_label = 'republican'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f724e4",
   "metadata": {},
   "source": [
    "## Conclusion and recap\n",
    "\n",
    "    Scikit-learn offers many classification models that can be grouped into two families:\n",
    "\n",
    "            Linear models like LogisticRegression.\n",
    "\n",
    "            Non-linear models like KNeighborsClassifier.\n",
    "\n",
    "    The implementation of these models is done in the same way for all models of scikit-learn:\n",
    "\n",
    "            Instantiation of the model.\n",
    "\n",
    "            Training of the model: model.fit(X_train, y_train).\n",
    "\n",
    "            Prediction: model.predict(X_test).\n",
    "\n",
    "    The prediction on the test set allows us to evaluate the performance of the model thanks to suitable metrics.\n",
    "\n",
    "    The metrics we have seen are used for binary classification and are calculated using 4 values:\n",
    "\n",
    "            True Positives: Prediction = + | Reality = +\n",
    "\n",
    "            True Negatives: Prediction = - | Reality =-\n",
    "\n",
    "            False Positives: Prediction = + |Reality = -\n",
    "\n",
    "            False Negatives: Prediction = -| Reality = +\n",
    "\n",
    "    All these values can be calculated using the confusion matrix generated by the confusion_matrix function of the sklearn.metrics submodule or by the pd.crosstab function.\n",
    "\n",
    "    Thanks to these values, we can calculate metrics like:\n",
    "\n",
    "            Accuracy: The proportion of correctly classified observations.\n",
    "\n",
    "            Precision: The proportion of true positives among all the positive predictions of the model.\n",
    "\n",
    "            Recall: the proportion of truly positive observations that were correctly classified as positive by the model.\n",
    "\n",
    "    All these metrics can be obtained using the classification_report function of the sklearn.metrics submodule.\n",
    "\n",
    "    The F1-Score quantifies the balance between these metrics, which gives us a reliable criterion for choosing the model most suited to our problem.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7375ece",
   "metadata": {},
   "source": [
    "## RECAP Classification KKN, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d709d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECAP Linear Regression \n",
    "# Load Data\n",
    "df = pd.read_csv('automobiles.csv', sep=',', index_col = 0)\n",
    "\n",
    "\n",
    "# Import Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "\n",
    "\n",
    "# Explanatory variables\n",
    "X = df.drop(['target'], axis = 1)\n",
    "\n",
    "\n",
    "# Target variable\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "# Splitting the dataset into a training set (85%) and a test set (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "\n",
    "#--------- KNN\n",
    "# Instantiation of the model\n",
    "knn = KNeighborsClassifier(n_neighbors = 6)\n",
    "\n",
    "# Training the model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Prediction of the target variable for the TEST dataset\n",
    "y_pred_test_knn = knn.predict(X_test)\n",
    "\n",
    "\n",
    "# -------- Confusion Matrix\n",
    "# Computation and display of the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test_knn)\n",
    "\n",
    "# Computation of the accuracy, precision and recall\n",
    "(TN, FP), (FN, TP) = confusion_matrix(y_test, y_pred_test_knn)\n",
    "n = len(y_test)\n",
    "\n",
    "print(\"\\nKNN Accuracy:\", (TP + TN) / n)\n",
    "\n",
    "print(\"\\nKNN Precision:\", TP / (TP + FP))\n",
    "\n",
    "print(\"\\nKNN Recall:\", TP / (TP + FN))\n",
    "\n",
    "# -------- Classification Report\n",
    "print(\"LogReg report:\\n\", classification_report(y_test, y_pred_test_logreg))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"KNN report:\\n\", classification_report(y_test, y_pred_test_knn))\n",
    "\n",
    "\n",
    "# -------- F1-Score\n",
    "print(\"F1 KNN:\", f1_score(y_test, y_pred_test_knn, pos_label = 'republican'))\n",
    "\n",
    "print(\"F1 LogReg:\", f1_score(y_test, y_pred_test_logreg, pos_label = 'republican'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------LogisticRegression\n",
    "# Instantiation of the model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Training the model\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Prediction of the target variable for the TEST dataset\n",
    "y_pred_test_logreg = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "# -------- Confusion Matrix\n",
    "# Computation and display of the confusion matrix\n",
    "pd.crosstab(y_test, y_pred_test_logreg, rownames=['Reality'], colnames=['Prediction'])\n",
    "\n",
    "# Computation of the accuracy, precision and recall\n",
    "print(\"\\nLogReg Accuracy:\", accuracy_score(y_test, y_pred_test_logreg))\n",
    "\n",
    "print(\"\\nLogReg Precision:\", precision_score(y_test, y_pred_test_logreg, pos_label = 'republican'))\n",
    "\n",
    "print(\"\\nLogReg Recall:\", recall_score(y_test, y_pred_test_logreg, pos_label = 'republican'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c611f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scpraing_portfolio_deneme",
   "language": "python",
   "name": "web_scpraing_portfolio_deneme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
