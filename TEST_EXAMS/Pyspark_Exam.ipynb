{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354bc060-934c-46bd-a1d9-80cdee16d288",
   "metadata": {},
   "source": [
    "\n",
    "Exam\n",
    "\n",
    "Big data processing with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751aaa92-1844-4b7f-818e-b8781ae90d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Exam\n",
    "\n",
    "Big data processing with PySpark\n",
    "\n",
    "    The exercise is composed of several questions, do them in order and be careful to respect the names of the variables. If you have any problems, feel free to contact the team DataScientest at help@datascientest.com\n",
    "\n",
    "    The purpose of this exercise is to implement and test the random forest algorithm in PySpark. A brief description of the algorithm and the data set is given below.\n",
    "    Data set\n",
    "\n",
    "    The data set for this exercise concerns the parameters for classifying banking transactions. It contains transactions carried out in September 2013 by European cardholders: 492 fraudulent transactions out of 284,807 transactions. The data set is very unbalanced, with positive classes (frauds) accounting for 0.172% of all transactions. The explanatory variables (features) are stored in the first thirty columns and the target variables (labels) in the last column.\n",
    "    Principle of random forests\n",
    "\n",
    "    Random forests are composed of a set of decision trees. These trees are distinguished from each other by the subsample of data on which they are trained. These sub-samples are randomly drawn from the initial data set.\n",
    "\n",
    "    The principle of random forest operation is simple: many small classification trees are produced on a random fraction of data. Random Forest has the classification trees voted on, in order to deduce the order and importance of the explanatory variables.\n",
    "\n",
    "    Build a Spark session named spark\n",
    "\n",
    "  Remember that before you look at the solution, you always have access to the official Python help by typing help(function_name) in the console.\n",
    "\n",
    "# Importing SparkSession and SParkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "​\n",
    "\n",
    "# Creating a SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "​\n",
    "\n",
    "# Creating a Spark session\n",
    "\n",
    "spark = SparkSession \\\n",
    "\n",
    "    .builder \\\n",
    "\n",
    "    .appName(\"Exam\") \\\n",
    "\n",
    "    .getOrCreate()\n",
    "\n",
    "        \n",
    "\n",
    "spark\n",
    "\n",
    "​\n",
    "\n",
    "SparkSession - in-memory\n",
    "\n",
    "SparkContext\n",
    "\n",
    "Spark UI\n",
    "\n",
    "Version\n",
    "    v3.1.2\n",
    "Master\n",
    "    local[*]\n",
    "AppName\n",
    "    pyspark-shell\n",
    "\n",
    "    Import the file creditcard.csv as a DataFrame called df_raw\n",
    "\n",
    "    View an extract from the DataFrame df_raw\n",
    "\n",
    "df_raw = spark.read.csv('creditcard.csv', header=True)\n",
    "\n",
    "​\n",
    "\n",
    "df_raw.sample(False, .00001, seed = 222).toPandas()\n",
    "\n",
    "\tTime \tV1 \tV2 \tV3 \tV4 \tV5 \tV6 \tV7 \tV8 \tV9 \t... \tV21 \tV22 \tV23 \tV24 \tV25 \tV26 \tV27 \tV28 \tAmount \tClass\n",
    "0 \t81398 \t-2.9215440526462 \t2.12176766319242 \t0.428956546002243 \t-1.2250003734459 \t-0.797057342087444 \t2.02977655328564 \t-3.80928678431259 \t-8.07475693383773 \t0.292737157796423 \t... \t-3.31385854289135 \t0.339413031176969 \t0.421792421426118 \t-1.03262272105476 \t-0.143543044112849 \t0.804582386656705 \t-0.203071921913024 \t-0.0632427975417301 \t14 \t0\n",
    "1 \t125631 \t2.05167792598642 \t0.01492155202324 \t-1.83062028734184 \t0.243623075945551 \t0.513547794304333 \t-0.414581378891218 \t-0.00235072356827228 \t-0.00447878134970368 \t0.430709328789236 \t... \t-0.345623462163563 \t-0.982075146136231 \t0.304552120789973 \t0.153942503687493 \t-0.288165115688497 \t0.17642002265289 \t-0.0745489709020419 \t-0.046091273362044 \t1.98 \t0\n",
    "2 \t131045 \t-0.199689743771287 \t0.118105016574984 \t1.23197163232753 \t0.0356107068743048 \t-0.268483901366946 \t0.588042264907413 \t0.172449623286078 \t0.181806370495549 \t-1.72398186778771 \t... \t-0.271763082582514 \t-0.504483360674054 \t0.407610418084526 \t0.593460290647732 \t-1.00095763621061 \t-0.922629463032307 \t0.251982303765905 \t0.230032971481475 \t93 \t0\n",
    "3 \t136155 \t-1.82364859874731 \t2.37392334829714 \t-1.20427130527347 \t-1.66565836873935 \t1.03138204531584 \t-1.48626765841278 \t1.97857162614601 \t-0.710835694187151 \t1.25793284010291 \t... \t-0.0925618033562254 \t1.00648032976056 \t-0.251248929187157 \t0.0423566783275898 \t-0.0305553026593055 \t-0.00675849383863872 \t0.452716574513226 \t-0.198752556659237 \t1.99 \t0\n",
    "4 \t165275 \t0.0840849882391582 \t1.01435280444105 \t-0.324490030592842 \t-0.640550840910365 \t0.982490291329449 \t-0.484930190328806 \t0.899821697199791 \t0.0244762106081547 \t-0.39535410555394 \t... \t-0.269414064743085 \t-0.634480662894263 \t0.101834454054891 \t0.615070703274599 \t-0.464440666810563 \t0.0905678575830634 \t0.224744561880514 \t0.0760685852892852 \t4.49 \t0\n",
    "\n",
    "5 rows × 31 columns\n",
    "\n",
    "    Create a DataFrame df from df_raw by changing the columns of the features to double and the target, Class, to int.\n",
    "\n",
    "    Display the schema of the variables of df\n",
    "      It is good practice to put the variable to be predicted or the target variable in the first column\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[:30]]\n",
    "\n",
    "​\n",
    "\n",
    "df = df_raw.select(df_raw.Class.cast('int'),\n",
    "\n",
    "                           *exprs)\n",
    "\n",
    "​\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "root\n",
    " |-- Class: integer (nullable = true)\n",
    " |-- Time: double (nullable = true)\n",
    " |-- V1: double (nullable = true)\n",
    " |-- V2: double (nullable = true)\n",
    " |-- V3: double (nullable = true)\n",
    " |-- V4: double (nullable = true)\n",
    " |-- V5: double (nullable = true)\n",
    " |-- V6: double (nullable = true)\n",
    " |-- V7: double (nullable = true)\n",
    " |-- V8: double (nullable = true)\n",
    " |-- V9: double (nullable = true)\n",
    " |-- V10: double (nullable = true)\n",
    " |-- V11: double (nullable = true)\n",
    " |-- V12: double (nullable = true)\n",
    " |-- V13: double (nullable = true)\n",
    " |-- V14: double (nullable = true)\n",
    " |-- V15: double (nullable = true)\n",
    " |-- V16: double (nullable = true)\n",
    " |-- V17: double (nullable = true)\n",
    " |-- V18: double (nullable = true)\n",
    " |-- V19: double (nullable = true)\n",
    " |-- V20: double (nullable = true)\n",
    " |-- V21: double (nullable = true)\n",
    " |-- V22: double (nullable = true)\n",
    " |-- V23: double (nullable = true)\n",
    " |-- V24: double (nullable = true)\n",
    " |-- V25: double (nullable = true)\n",
    " |-- V26: double (nullable = true)\n",
    " |-- V27: double (nullable = true)\n",
    " |-- V28: double (nullable = true)\n",
    " |-- Amount: double (nullable = true)\n",
    "\n",
    "    Delete lines containing missing values from the DataFrame df\n",
    "\n",
    "    Create a rdd rdd_ml separating the variable to be explained from the features (to be put in DenseVector form)\n",
    "\n",
    "    Create a DataFrame df_ml containing our database under two variables: 'labels' and 'features'\n",
    "\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "​\n",
    "\n",
    "null_counts = df.select([spark_sum(col(c).isNull().cast('int')).alias(c) for c in df.columns])\n",
    "\n",
    "​\n",
    "\n",
    "null_counts.show()\n",
    "\n",
    "+-----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+\n",
    "|Class|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|\n",
    "+-----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+\n",
    "|    0|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|\n",
    "+-----+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+\n",
    "\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "​\n",
    "\n",
    "# Conversion of the database to svmlib format\n",
    "\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "​\n",
    "\n",
    "df_ml.show(5)\n",
    "\n",
    "+-----+--------------------+\n",
    "|label|            features|\n",
    "+-----+--------------------+\n",
    "|    0|[0.0,-1.359807133...|\n",
    "|    0|[0.0,1.1918571113...|\n",
    "|    0|[1.0,-1.358354061...|\n",
    "|    0|[1.0,-0.966271711...|\n",
    "|    0|[2.0,-1.158233093...|\n",
    "+-----+--------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "    Create two DataFrames called train and test each containing 80% and 20% of the data respectively\n",
    "\n",
    "    Create a Random Forest classifier called clf\n",
    "\n",
    "    Train the random forest model on the training set\n",
    "\n",
    "  Learning from big data usually takes a long time to complete. The cell below is executed in two minutes in the worst case\n",
    "\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=222)\n",
    "\n",
    "​\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "​\n",
    "\n",
    "clf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "model = clf.fit(train)\n",
    "\n",
    "    Calculate the accuracy, accuracy, of the driven model\n",
    "\n",
    "#Calculation of predictions\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "​\n",
    "\n",
    "# Creation of an evaluator \n",
    "\n",
    "ev = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "\n",
    "                                              labelCol= 'label',\n",
    "\n",
    "                                              predictionCol= 'prediction')\n",
    "\n",
    "# Calculation and display of model accuracy \n",
    "\n",
    "accuracy = ev.evaluate(predictions)\n",
    "\n",
    "print(\"RandomForestClassifier Accuracy = \",accuracy)\n",
    "\n",
    "​\n",
    "\n",
    "RandomForestClassifier Accuracy =  0.9993335788569123\n",
    "\n",
    "# ------------------- Extras --------------------\n",
    "\n",
    "# Import of ParamGridBuilder from the pyspark.ml.tuning package\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "​\n",
    "\n",
    "# Creating a parameter grid\n",
    "\n",
    "param_grid = ParamGridBuilder().\\\n",
    "\n",
    "    addGrid(clf.numTrees, [5, 20, 1]).\\\n",
    "\n",
    "    build()\n",
    "\n",
    "    \n",
    "\n",
    "# Creation of an evaluator \n",
    "\n",
    "ev = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "\n",
    "                                              labelCol= 'label',\n",
    "\n",
    "                                              predictionCol= 'prediction')\n",
    "\n",
    "​\n",
    "\n",
    "# Creation of a 3-fold cross validator\n",
    "\n",
    "cv = CrossValidator(estimator = clf,\n",
    "\n",
    "                    estimatorParamMaps = param_grid,\n",
    "\n",
    "                    evaluator=ev,\n",
    "\n",
    "                    numFolds=3)\n",
    "\n",
    "​\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "# Calculation of predictions on test set \n",
    "\n",
    "predictions_cv = cv_model.transform(test)\n",
    "\n",
    "​\n",
    "\n",
    "# Calculation and display of model accuracy \n",
    "\n",
    "accuracy = ev.evaluate(predictions_cv)\n",
    "\n",
    "print(\"RandomForestClassifier Accuracy = \",accuracy)\n",
    "\n",
    "RandomForestClassifier Accuracy =  0.9993335788569123\n",
    "\n",
    "predictions_cv.show(5)\n",
    "\n",
    "+-----+--------------------+--------------------+--------------------+----------+\n",
    "|label|            features|       rawPrediction|         probability|prediction|\n",
    "+-----+--------------------+--------------------+--------------------+----------+\n",
    "|    0|[7.0,-0.894286082...|[19.9941118780554...|[0.99970559390277...|       0.0|\n",
    "|    0|[18.0,0.247491127...|[19.9941118780554...|[0.99970559390277...|       0.0|\n",
    "|    0|[23.0,-0.41428881...|[19.9941118780554...|[0.99970559390277...|       0.0|\n",
    "|    0|[23.0,1.173284610...|[19.9941118780554...|[0.99970559390277...|       0.0|\n",
    "|    0|[26.0,-0.53538776...|[19.9941118780554...|[0.99970559390277...|       0.0|\n",
    "+-----+--------------------+--------------------+--------------------+----------+\n",
    "only showing top 5 rows\n",
    "\n",
    "# Display of model coefficients\n",
    "\n",
    "print(cv_model.bestModel)\n",
    "\n",
    "RandomForestClassificationModel: uid=RandomForestClassifier_abe25c9391b3, numTrees=20, numClasses=2, numFeatures=30\n",
    "\n",
    "​"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (Visual Studio)",
   "language": "python",
   "name": "visualstudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
