{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7524d50b",
   "metadata": {},
   "source": [
    "''' Pyspark '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a32d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e82ee",
   "metadata": {},
   "source": [
    "'''Pyspark Deneme'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cd0ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/04 09:52:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The PySpark 3.4.0 version is running...\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\") \\\n",
    "                    .appName('BigData-ETL.com') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(f'The PySpark {spark.version} version is running...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36218c24",
   "metadata": {},
   "source": [
    "# Pysaprk DataScientest Ders01 - Big data processing with Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0a576-1291-4ba2-82b6-28bdf9e13e41",
   "metadata": {},
   "source": [
    "# DERS 01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c8c42-21bc-452e-845f-fd60081bce86",
   "metadata": {},
   "source": [
    "## Introduction to PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa23a27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/04 10:15:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing SparkContext from the pyspark module\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Defining a SparkContext locally\n",
    "sc = SparkContext('local')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60180baf-7ecd-41ca-ad2a-1539bb2e9f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 0.784 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import the time library and calculation of time at the beginning of the execution (t0)\n",
    "path = \"/Users/macbook/Desktop/IBM/DataScientest_Datalar\"\n",
    "\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "#Reading the file \"2008_raw.csv\"\n",
    "raw_rdd = sc.textFile(f\"file:///{path}/2008.csv\")\n",
    "#raw_rdd = sc.textFile(f\"/{path}/2008.csv\") ==> Hata veriyor file:/// eklemezsen \n",
    "\n",
    "# Calculation of file reading time\n",
    "t1 = time() - t0\n",
    "print(\"Done in {} seconds\".format(round(t1,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83dfbc49-b47d-43d5-82ba-f96650cbf569",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 2.192 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2008,1,1,2,2057,2052,2312,2258,AS,324,N306AS,135,126,112,14,5,SEA,SJC,697,7,16,0,,0,NA,NA,NA,NA,NA',\n",
       " '2008,1,1,2,703,715,958,951,AS,572,N302AS,175,156,144,7,-12,SEA,PSP,987,6,25,0,,0,NA,NA,NA,NA,NA',\n",
       " '2008,1,1,2,2011,1846,2248,2145,AS,511,N564AS,157,179,136,63,85,SAN,SEA,1050,7,14,0,,0,0,0,0,0,63',\n",
       " '2008,1,1,2,2301,2300,2354,2359,AS,376,N309AS,53,59,35,-5,1,SEA,GEG,224,5,13,0,,0,NA,NA,NA,NA,NA',\n",
       " '2008,1,1,2,1221,1221,1422,1438,AS,729,N317AS,181,197,164,-16,0,TUS,SEA,1216,6,11,0,,0,NA,NA,NA,NA,NA']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculation of time at the beginning of execution (t0)\n",
    "t0 = time()\n",
    "\n",
    "### Insert your code here\n",
    "raw_rdd.take(5)\n",
    "\n",
    "### Do not change the code below\n",
    "# Calculation of the time of display of the 5 elements\n",
    "t1 = time() - t0\n",
    "print(\"Done in {} seconds\".format(round(t1,3)))\n",
    "raw_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb5507e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151102"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculation of the number of lines \n",
    "count = raw_rdd.count()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "660ca23b-cb54-46c7-9888-fb97e338fb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2008',\n",
       "  '1',\n",
       "  '1',\n",
       "  '2',\n",
       "  '2057',\n",
       "  '2052',\n",
       "  '2312',\n",
       "  '2258',\n",
       "  'AS',\n",
       "  '324',\n",
       "  'N306AS',\n",
       "  '135',\n",
       "  '126',\n",
       "  '112',\n",
       "  '14',\n",
       "  '5',\n",
       "  'SEA',\n",
       "  'SJC',\n",
       "  '697',\n",
       "  '7',\n",
       "  '16',\n",
       "  '0',\n",
       "  '',\n",
       "  '0',\n",
       "  'NA',\n",
       "  'NA',\n",
       "  'NA',\n",
       "  'NA',\n",
       "  'NA']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an rdd whose lines are a list of raw_rdd elements \n",
    "airplane_rdd = raw_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Display of the first line of the rdd\n",
    "airplane_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b31e0e-1c54-4f89-992e-6d2a6a3a90f6",
   "metadata": {},
   "source": [
    "## Map, reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a38cd73-103c-4a9b-a66c-6842e3f32729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2258', 89), ('951', 156), ('2145', 314), ('2359', 223), ('1438', 106)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a new rdd by summarizing the lines by the departure airport\n",
    "hist_rdd = airplane_rdd.map(lambda x: (x[7], 1)).reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "# Display of the first 5 lines \n",
    "hist_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d06eb1-0f24-4f82-b809-8de7690e1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2008,1,1,2,2057,2052,2312,2258,AS,324,N306AS,135,126,112,14,5,SEA,SJC,697,7,16,0,,0,NA,NA,NA,NA,NA']\n"
     ]
    }
   ],
   "source": [
    "# Step 02\n",
    "print(raw_rdd.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27f7e906-c679-4f2b-a098-4523887ae33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hist_rdd =  [('2258', 1), ('951', 1), ('2145', 1)]\n",
      "hist_raw_rdd =  [('2008,1,1,2', 1), ('2008,1,1,2', 1), ('2008,1,1,2', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Step 03\n",
    "hist_rdd = airplane_rdd.map(lambda line: (line[7], 1))\n",
    "hist_raw_rdd = raw_rdd.map(lambda line_string: (line_string[0:10], 1))\n",
    "print(\"hist_rdd = \", hist_rdd.take(3))\n",
    "print(\"hist_raw_rdd = \", hist_raw_rdd.take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5ad375-3070-4283-b76a-2c7591996e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2008,', '2008,', '2008,', '2008,', '2008,']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 04\n",
    "sil = raw_rdd.map(lambda line: (line[0:5]))\n",
    "sil.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a8444-ffbb-4c16-8f3a-ddd313a3bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60de459c-4401-479f-b6cc-e01bd7b3b7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2258', 1), ('951', 1), ('2145', 1), ('2359', 1), ('1438', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert your code here\n",
    "hist_rdd.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f1f7c-7290-4aa1-a576-67e2f4a28a41",
   "metadata": {},
   "source": [
    "## sorted ,reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f333acd3-e0e6-4dde-a90e-0e30a253af76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2258', 1), ('951', 1), ('2145', 1), ('2359', 1), ('1438', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('951', 1),\n",
       " ('844', 1),\n",
       " ('523', 1),\n",
       " ('2359', 1),\n",
       " ('2330', 1),\n",
       " ('2258', 1),\n",
       " ('2145', 1),\n",
       " ('2125', 1),\n",
       " ('1956', 1),\n",
       " ('1438', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sorting data in decreasing order\n",
    "# Creating a list from an rdd\n",
    "hist = hist_rdd.collect()\n",
    "\n",
    "# display of the entire list\n",
    "print(hist[:5])\n",
    "\n",
    "sorted(hist[:10], key= lambda x: x[0], reverse= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048927e3-9bbd-47c0-864b-b3701f01e689",
   "metadata": {},
   "source": [
    "## filter, map, reduceByKey, collect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75adc1a3-a0fb-4905-b7e3-809686ac85eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an rdd whose lines are a list of raw_rdd elements \n",
    "airplane_rdd = raw_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Calculation and display of the number of cancelled flights by city of origin\n",
    "airplane_rdd \\\n",
    "    .filter(lambda x: x[10] == \"1\") \\\n",
    "    .map(lambda x: (x[8], 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b84f003-8424-49ff-be2e-7346047908b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airplane_rdd \\\n",
    "    .filter(lambda x: x[10] == \"1\") \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16bfe01c-c761-4b97-93fe-616486f7da2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2008',\n",
       "  '1',\n",
       "  '1',\n",
       "  '2',\n",
       "  '2057',\n",
       "  '2052',\n",
       "  '2312',\n",
       "  '2258',\n",
       "  'AS',\n",
       "  '324',\n",
       "  'N306AS',\n",
       "  '135',\n",
       "  '126',\n",
       "  '112',\n",
       "  '14',\n",
       "  '5',\n",
       "  'SEA',\n",
       "  'SJC',\n",
       "  '697',\n",
       "  '7',\n",
       "  '16',\n",
       "  '0',\n",
       "  '',\n",
       "  '0',\n",
       "  'NA',\n",
       "  'NA',\n",
       "  'NA',\n",
       "  'NA',\n",
       "  'NA']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airplane_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972c8481-b2b0-46f7-ad22-615f0835f469",
   "metadata": {},
   "source": [
    "# DERS 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "234cfaf7-827a-4b6e-a8d5-7a14ebeeada5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'LES MISÉRABLES',\",\n",
       " \" 'VOLUME I.-FANTINE.',\",\n",
       " \" 'PREFACE',\",\n",
       " \" 'So long as there shall exist, by virtue of law and custom, decrees of damnation pronounced by society, artificially creating hells amid the civilization of earth, and adding the element of human fate to divine destiny ; so long as the three great problems of the century-the degradation of man through pauperism, the corruption of woman through hunger, the crippling of children through lack of light-are unsolved ; so long as social asphyxia is possible in any part of the world ;-in other words, and with a still wider significance, so long as ignorance and poverty exist on earth, books of the nature of Les Misérables cannot fail to be of use.',\",\n",
       " \" 'HAUTEVILLE HOUSE, 1862.',\"]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miserables = sc.textFile(f\"file:///{path}/miserable_full.txt\")\n",
    "miserables.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7e014-53c8-42cf-8e1d-73fffcf48565",
   "metadata": {},
   "source": [
    "## replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "972654a6-b3d7-49ae-984c-5a17a1e93963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' les misérables  ',\n",
       " '  volume i  fantine   ',\n",
       " '  preface  ',\n",
       " '  so long as there shall exist  by virtue of law and custom  decrees of damnation pronounced by society  artificially creating hells amid the civilization of earth  and adding the element of human fate to divine destiny ; so long as the three great problems of the century the degradation of man through pauperism  the corruption of woman through hunger  the crippling of children through lack of light are unsolved ; so long as social asphyxia is possible in any part of the world ; in other words  and with a still wider significance  so long as ignorance and poverty exist on earth  books of the nature of les misérables cannot fail to be of use   ',\n",
       " '  hauteville house  1862   ']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miserables_clean = miserables.map(lambda x : x.lower().replace(',', ' ')\\\n",
    "                                  .replace('.', ' ')\\\n",
    "                                  .replace('-', ' ')\\\n",
    "                                  .replace('\\'', ' '))\n",
    "miserables_clean.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e87128c-474c-40c7-b24b-18b63091cdd8",
   "metadata": {},
   "source": [
    "## flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "15efc2c8-82d0-433c-8553-25dba95073d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miserables_flat count =  54702\n",
      "miserables count = 1001\n"
     ]
    }
   ],
   "source": [
    "miserables_flat = miserables_clean.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"miserables_flat count = \", miserables_flat.count())\n",
    "print(\"miserables count =\", miserables.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3af2bccc-0cab-47d9-8cb4-df0bee3fa6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 10405), ('the', 3015), ('of', 1564), ('a', 1136), ('and', 1097)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = miserables_flat.map(lambda x : (x, 1)) \\\n",
    "                        .reduceByKey(lambda x,y : x+y) \\\n",
    "                        .collect()\n",
    "\n",
    "sorted(words, key=lambda x: x[1], reverse=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048bb90-3b3a-42fa-a444-54bb35a4353f",
   "metadata": {},
   "source": [
    "## all func. in one code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30f3ad3c-89c1-41d7-8712-f72e086f0da0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 10405), ('the', 3015), ('of', 1564), ('a', 1136), ('and', 1097)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Direct creation of a list containing the words\n",
    "words_sorted_3 =  sc.textFile(f\"file:///{path}/miserable_full.txt\") \\\n",
    "                    .map(lambda x : x.lower().replace(',', ' ').replace('.', ' ').replace('-', ' ').replace('\\'', ' ').replace('\\\\', ' ')) \\\n",
    "                    .flatMap(lambda line : line.split(\" \")) \\\n",
    "                    .map(lambda x : (x,1)) \\\n",
    "                    .reduceByKey(lambda x,y : x+y) \\\n",
    "                    .sortBy(lambda couple: couple[1], ascending = False) \\\n",
    "\n",
    "words_sorted_3.collect()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678bbd5-ca90-439d-8f1e-72b74feeeeb9",
   "metadata": {},
   "source": [
    "# DERS 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4df201-e619-4d62-9b66-c8565efb4bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/04 20:35:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8b0528c280>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Spark Session and SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Definition of a SparkContext\n",
    "SparkContext.getOrCreate() \n",
    "\n",
    "# Definition of a SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction to DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a665e2f1-39f4-462c-ac46-1edffeb60b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-mbp:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a shortcut to the SparkContext already created\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e528f9-f99a-416d-8bb5-d7dfaff402de",
   "metadata": {},
   "source": [
    "## Import Row, create DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1211f720-893d-499c-9832-fce0da2fca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 20:39:02 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 1 (TID 1): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+---------+\n",
      "|year|month|day|flightNum|\n",
      "+----+-----+---+---------+\n",
      "|2008|    1|  1|     2052|\n",
      "|2008|    1|  1|      715|\n",
      "|2008|    1|  1|     1846|\n",
      "|2008|    1|  1|     2300|\n",
      "|2008|    1|  1|     1221|\n",
      "+----+-----+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Import the Row structure from the module pyspark.sql\n",
    "Import the database 2008_raw.csv\n",
    "Create a rdd from this database\n",
    "Create a rdd_row using the map method, applying on each line the structure Row with the explanatory variables year, month, day and flightNum\n",
    "Create a DataFrame df from rdd_row\n",
    "'''\n",
    "\n",
    "path = \"/Users/macbook/Desktop/IBM/DataScientest_Datalar\"\n",
    "\n",
    "# Importing Row from the pyspark.sql package\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Loading the file '2008_raw.csv'\n",
    "rdd = sc.textFile(f'file:///{path}/2008.csv').map(lambda line: line.split(\",\"))\n",
    "\n",
    "# Creating a new rdd by selecting the explanatory variables\n",
    "rdd_row = rdd.map(lambda line: Row(year = line[0],\n",
    "                                   month = line[1],\n",
    "                                   day = line[2],\n",
    "                                   flightNum = line[5]))\n",
    "\n",
    "# Creating a data frame from an rdd\n",
    "df = spark.createDataFrame(rdd_row)\n",
    "\n",
    "\n",
    "# Display of the first 5 lines\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4045ad93-c5c0-432d-aecc-40d2c3a537a4",
   "metadata": {},
   "source": [
    "## Reading csv file, printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d29354cb-deb7-4e7c-9ec4-237b10e2ecfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      " |-- _c20: string (nullable = true)\n",
      " |-- _c21: string (nullable = true)\n",
      " |-- _c22: string (nullable = true)\n",
      " |-- _c23: string (nullable = true)\n",
      " |-- _c24: string (nullable = true)\n",
      " |-- _c25: string (nullable = true)\n",
      " |-- _c26: string (nullable = true)\n",
      " |-- _c27: string (nullable = true)\n",
      " |-- _c28: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading the file '2008.csv'\n",
    "raw_df = spark.read.csv(f'file:///{path}/2008.csv', header=False)\n",
    "\n",
    "# Display of the variables' schema\n",
    "raw_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdcb95e7-6099-48f8-b50e-6d261fe0883b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+---+----+----+----+----+---+---+------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| _c0|_c1|_c2|_c3| _c4| _c5| _c6| _c7|_c8|_c9|  _c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|\n",
      "+----+---+---+---+----+----+----+----+---+---+------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2008|  1|  1|  2|2057|2052|2312|2258| AS|324|N306AS| 135| 126| 112|  14|   5| SEA| SJC| 697|   7|  16|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2| 703| 715| 958| 951| AS|572|N302AS| 175| 156| 144|   7| -12| SEA| PSP| 987|   6|  25|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2|2011|1846|2248|2145| AS|511|N564AS| 157| 179| 136|  63|  85| SAN| SEA|1050|   7|  14|   0|null|   0|   0|   0|   0|   0|  63|\n",
      "|2008|  1|  1|  2|2301|2300|2354|2359| AS|376|N309AS|  53|  59|  35|  -5|   1| SEA| GEG| 224|   5|  13|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2|1221|1221|1422|1438| AS|729|N317AS| 181| 197| 164| -16|   0| TUS| SEA|1216|   6|  11|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2|1843|1840|2110|2125| AS|283|N318AS| 147| 165| 124| -15|   3| LAX| SEA| 954|   7|  16|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2|2045|2045|2314|2330| AS|211|N305AS| 149| 165| 126| -16|   0| LAX| SEA| 954|   6|  17|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2|  49|  50| 547| 523| AS|100|N315AS| 238| 213| 222|  24|  -1| ANC| PDX|1542|   2|  14|   0|null|   0|   0|   0|  24|   0|   0|\n",
      "|2008|  1|  1|  2|1719|1715|1939|1956| AS|665|N302AS| 140| 161| 118| -17|   4| LAS| SEA| 866|   8|  14|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2| 613| 630| 815| 844| AS|531|N755AS| 122| 134|  96| -29| -17| SJC| SEA| 697|   7|  19|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2| 753| 720|1125|1103| AS|571|N320AS| 152| 163| 124|  22|  33| SEA| DEN|1024|   5|  23|   0|null|   0|  22|   0|   0|   0|   0|\n",
      "|2008|  1|  1|  2|null| 205|null| 620| AS|154|N309AS|null| 195|null|null|null| ANC| SEA|1449|null|null|   1|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2| 741| 740|1128|1139| AS|728|N317AS| 167| 179| 150| -11|   1| SEA| TUS|1216|   4|  13|   0|null|   0|null|null|null|null|null|\n",
      "|2008|  1|  1|  2|1702|1530|1941|1804| AS|518|N564AS| 159| 154| 142|  97|  92| SEA| SAN|1050|   2|  15|   0|null|   0|  92|   0|   5|   0|   0|\n",
      "|2008|  1|  1|  2|1306|1245|1551|1525| AS|580|N307AS| 165| 160| 142|  26|  21| SEA| SAN|1050|   2|  21|   0|null|   0|  21|   0|   5|   0|   0|\n",
      "+----+---+---+---+----+----+----+----+---+---+------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d16aeea5-ad59-4fcf-bba5-4b1355e6ce40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|2008|    1|  1|2057|      324| N306AS|   SEA| SJC| 697|       0|\n",
      "|2008|    1|  1| 703|      572| N302AS|   SEA| PSP| 987|       0|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 21:09:51 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 40 (TID 42): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a new rdd by selecting the explanatory variables\n",
    "rdd_row = rdd.map(lambda line: Row(year = line[0],\n",
    "                                   month = line[1],\n",
    "                                   day = line[2],\n",
    "                                   hour = line[4],\n",
    "                                   flightNum = line[9],\n",
    "                                   tailNum = line[10],\n",
    "                                   origin = line[16],\n",
    "                                   dest = line[17],\n",
    "                                   dist = line[18],\n",
    "                                   canceled = line[21]\n",
    "                                ))\n",
    "\n",
    "rdd_row.take(2)\n",
    "\n",
    "df = spark.createDataFrame(rdd_row)\n",
    "\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07dfacf9-fcee-4e93-be71-db32fe49ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- flightNum: string (nullable = true)\n",
      " |-- tailNum: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dist: string (nullable = true)\n",
      " |-- canceled: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276173d-40d8-42c9-8bbd-1e0d7e695979",
   "metadata": {},
   "source": [
    "## change col type 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4f737f5-b9d0-4c9a-a91f-a940ff2122ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: string (nullable = true)\n",
      " |-- flightNum: integer (nullable = true)\n",
      " |-- tailNum: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- dist: integer (nullable = true)\n",
      " |-- canceled: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "integerColumns = integerList = ['year', 'month', 'day', 'flightNum', 'dist', 'canceled' ]\n",
    "\n",
    "stringColumns = ['tailNum','origin', 'dest']\n",
    "\n",
    "# integerColumns listesindeki sütunları integer tipine dönüştürmek\n",
    "for col_name in integerColumns:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(\"int\"))\n",
    "\n",
    "# stringColumns tuple'ındaki sütunları string tipine dönüştürmek\n",
    "for col_name in stringColumns:\n",
    "    df = df.withColumn(col_name, col(col_name).cast(\"string\"))\n",
    "    \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705f6f97-b3e7-4032-a983-7b38ea48f620",
   "metadata": {},
   "source": [
    "## change col type 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c17478-e78b-4c25-9113-53c6b40d3ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a data frame by specifying the type of columns\n",
    "flights = raw_df.select(raw_df.year.cast(\"int\"),\n",
    "                        raw_df.month.cast(\"int\"),\n",
    "                        raw_df.day.cast(\"int\"),\n",
    "                        raw_df.flightNum.cast(\"int\"),\n",
    "                        raw_df.origin.cast(\"string\"),\n",
    "                        raw_df.dest.cast(\"string\"),\n",
    "                        raw_df.distance.cast(\"int\"),\n",
    "                        raw_df.canceled.cast(\"boolean\"),\n",
    "                        raw_df.cancellationCode.cast(\"string\"),\n",
    "                        raw_df.carrierDelay.cast(\"int\"))\n",
    "\n",
    "# Display of 20 first lines\n",
    "flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aaadaf-7a37-4c85-a170-2e98d1e37c23",
   "metadata": {},
   "source": [
    "## describe , toPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2134c172-771a-460c-abe6-fe629b77b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------+--------+--------+---------+-------+------+------+--------+--------+\n",
      "|summary|    year|   month|     day|    hour|flightNum|tailNum|origin|  dest|    dist|canceled|\n",
      "+-------+--------+--------+--------+--------+---------+-------+------+------+--------+--------+\n",
      "|  count|  151102|  151102|  151102|  151102|   151102| 151102|151102|151102|  151102|  151102|\n",
      "|   mean|  2008.0|6.414...|15.70...|1333....| 336.7...|   null|  null|  null|957.9...|0.014...|\n",
      "| stddev|1.136...|3.372...|8.794...|511.2...| 235.5...|   null|  null|  null|598.6...|0.118...|\n",
      "|    min|    2008|       1|       1|       1|        1| N302AS|   ADK|   ADK|      31|       0|\n",
      "|    max|    2008|      12|      31|      NA|      997| N982AS|   YAK|   YAK|    2846|       1|\n",
      "+-------+--------+--------+--------+--------+---------+-------+------+------+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>flightNum</th>\n",
       "      <th>tailNum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>dist</th>\n",
       "      <th>canceled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "      <td>151102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>6.414633823509947</td>\n",
       "      <td>15.70080475440431</td>\n",
       "      <td>1333.6065953390969</td>\n",
       "      <td>336.75316011700704</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>957.9093526227317</td>\n",
       "      <td>0.014156000582388056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>1.1368673191536325E-13</td>\n",
       "      <td>3.372586753808719</td>\n",
       "      <td>8.79497003328945</td>\n",
       "      <td>511.2859647572262</td>\n",
       "      <td>235.5397135668841</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>598.6289977832145</td>\n",
       "      <td>0.11813424816440472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N302AS</td>\n",
       "      <td>ADK</td>\n",
       "      <td>ADK</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>NA</td>\n",
       "      <td>997</td>\n",
       "      <td>N982AS</td>\n",
       "      <td>YAK</td>\n",
       "      <td>YAK</td>\n",
       "      <td>2846</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                    year              month                day   \n",
       "0   count                  151102             151102             151102  \\\n",
       "1    mean                  2008.0  6.414633823509947  15.70080475440431   \n",
       "2  stddev  1.1368673191536325E-13  3.372586753808719   8.79497003328945   \n",
       "3     min                    2008                  1                  1   \n",
       "4     max                    2008                 12                 31   \n",
       "\n",
       "                 hour           flightNum tailNum  origin    dest   \n",
       "0              151102              151102  151102  151102  151102  \\\n",
       "1  1333.6065953390969  336.75316011700704    None    None    None   \n",
       "2   511.2859647572262   235.5397135668841    None    None    None   \n",
       "3                   1                   1  N302AS     ADK     ADK   \n",
       "4                  NA                 997  N982AS     YAK     YAK   \n",
       "\n",
       "                dist              canceled  \n",
       "0             151102                151102  \n",
       "1  957.9093526227317  0.014156000582388056  \n",
       "2  598.6289977832145   0.11813424816440472  \n",
       "3                 31                     0  \n",
       "4               2846                     1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying a summary using the truncate option of the show method\n",
    "df.describe().show(truncate = 8)\n",
    "\n",
    "### Second method\n",
    "# Displaying a summary using the method toPandas\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda5cde4-e6b3-428a-839b-722aa3d8fcca",
   "metadata": {},
   "source": [
    "## distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa68bed-2bab-4a69-9c69-4c07daf4ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "681"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculation of the number of flights with separate flight numbers\n",
    "df.select('flightNum').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b01b7-e47e-4b4f-86fb-6140714acbcc",
   "metadata": {},
   "source": [
    "## groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95fb5728-cb46-409e-9854-2c288b3601cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|canceled| count|\n",
      "+--------+------+\n",
      "|       1|  2139|\n",
      "|       0|148963|\n",
      "+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display of the summary of the category variable \"cancellationCode\".\n",
    "\n",
    "df.groupBy('canceled').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19203cff-36ba-4f76-b265-559fbdcaf777",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "801c1f69-7cf1-4fab-82fc-79e923e06bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|2008|    1|  1|  NA|      154| N309AS|   ANC| SEA|1449|       1|\n",
      "|2008|    1|  1|  NA|      327| N792AS|   SNA| PDX| 859|       1|\n",
      "|2008|    1|  1|  NA|      488| N792AS|   PDX| SNA| 859|       1|\n",
      "|2008|    1|  1|  NA|      464| N960AS|   SEA| ONT| 956|       1|\n",
      "|2008|    1|  1|  NA|      631| N962AS|   LAS| SEA| 866|       1|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/04 21:13:47 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 55 (TID 57): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(df.canceled == 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8245c5-c186-47cc-96e3-9bc98085d830",
   "metadata": {},
   "source": [
    "## filter, groupBy, count, orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab455968-d8b8-4234-9833-4669a4de7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|   11|   65|\n",
      "|    9|   67|\n",
      "|    3|   85|\n",
      "|   10|   93|\n",
      "|    7|   98|\n",
      "|    6|  104|\n",
      "|    5|  127|\n",
      "|    8|  154|\n",
      "|    4|  158|\n",
      "|    2|  206|\n",
      "|    1|  355|\n",
      "|   12|  627|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(df.canceled == 1).groupBy('month').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e02a4e6-14b3-4cbd-96b6-0d263b873f65",
   "metadata": {},
   "source": [
    "## withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df402e76-3b39-435b-9f28-342c1abea511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 73:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+------------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|isLongFlight|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+------------+\n",
      "|2008|    1|  1|2057|      324| N306AS|   SEA| SJC| 697|       0|       false|\n",
      "|2008|    1|  1| 703|      572| N302AS|   SEA| PSP| 987|       0|       false|\n",
      "|2008|    1|  1|2011|      511| N564AS|   SAN| SEA|1050|       0|        true|\n",
      "|2008|    1|  1|2301|      376| N309AS|   SEA| GEG| 224|       0|       false|\n",
      "|2008|    1|  1|1221|      729| N317AS|   TUS| SEA|1216|       0|        true|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 03:58:06 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 73 (TID 75): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creation of a new variable \"isLongFlight\" and display of the first 10 lines\n",
    "\n",
    "df.withColumn('isLongFlight', df.dist > 1000).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79c2c041-e2d1-4f0a-a681-b8148c939706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "151102"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "779dd1a3-3d15-46c5-b04f-f72695e03168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46574"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter( df.dist > 1000).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8619c539-21bf-49a2-b560-7b07c0df44d0",
   "metadata": {},
   "source": [
    "## fillna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e960de6-7d34-4977-b0ef-115ddab9af28",
   "metadata": {},
   "source": [
    "df.fillna( newValue, 'columnName') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "37aabde3-80c8-4f8e-b56d-9eb8c730ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.filter(df.hour.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d254685-6dc5-4913-8231-cdd43dad84b7",
   "metadata": {},
   "source": [
    "## when, otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6b3f303e-d9fc-4ec6-9b17-402c563b5cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|2008|    1|  1|null|      154| N309AS|   ANC| SEA|1449|       1|\n",
      "|2008|    1|  1|null|      327| N792AS|   SNA| PDX| 859|       1|\n",
      "|2008|    1|  1|null|      488| N792AS|   PDX| SNA| 859|       1|\n",
      "|2008|    1|  1|null|      464| N960AS|   SEA| ONT| 956|       1|\n",
      "|2008|    1|  1|null|      631| N962AS|   LAS| SEA| 866|       1|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 04:09:55 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 87 (TID 89): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Convert 'NA' values to NULL in the 'hour' column\n",
    "df = df.withColumn('hour', when(col('hour') == 'NA', None).otherwise(col('hour').cast('integer')))\n",
    "\n",
    "# Filter rows where 'hour' is NULL and show the first 5 rows\n",
    "df.filter(df.hour.isNull()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3527c3-7584-44ce-a023-0821be6efddd",
   "metadata": {},
   "source": [
    "## isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466bcd32-52c9-43b4-87a1-54d682920621",
   "metadata": {},
   "source": [
    "kolonlarin Null deger toplamini bulma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9e98f03a-9bed-4a04-90e7-b8a586af2afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|   0|    0|  0|2118|        0|      0|     0|   0|   0|       0|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Toplam NULL değer içeren sütunları bulma\n",
    "null_counts = df.select([spark_sum(col(c).isNull().cast('integer')).alias(c) for c in df.columns])\n",
    "\n",
    "# Sonuçları gösterme\n",
    "null_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd9a7ca-29c5-49cc-ba53-3bf7efc2f411",
   "metadata": {},
   "source": [
    "## replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563bbcb4-c199-4ed9-a6d8-0ca19f324b86",
   "metadata": {},
   "source": [
    "df.replace(oldValue, newValue)\n",
    "replaces on the whole database\n",
    "\n",
    "df.replace(oldValue, newValue, 'columnName')\n",
    "replaces only on the specified columns\n",
    "\n",
    "df.replace([oldValue1, oldValue2], [newValue1, newValue2], 'columnName')\n",
    "if several values to be replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfd4299-27d2-4636-b136-ae44ad6a8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Replace, in order, the cancellation codes 'A','B','C' with '1', '2', '3'\n",
    "\n",
    "df.replace(['A', 'B', 'C'], [1, 2, 3], ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac5339-a144-4b00-8d2b-c56c8eaa9b59",
   "metadata": {},
   "source": [
    "## order by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb49bc-148f-458c-82b1-d9d2a35f5265",
   "metadata": {},
   "source": [
    "df.orderBy(df.age)\n",
    "ordered by the variable 'age'\n",
    "\n",
    "df.orderBy(df.age.desc())\n",
    "order in decreasing order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "39aea550-3cf8-4951-a0ca-f5f71dfbcac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 89:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|year|month|day|hour|flightNum|tailNum|origin|dest|dist|canceled|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "|2008|    8| 29|1255|      997| N708AS|   SEA| ANC|1449|       0|\n",
      "|2008|    6|  7|2057|      996| N562AS|   SEA| GEG| 224|       0|\n",
      "|2008|    8| 25| 800|      991| N570AS|   DEN| SEA|1024|       0|\n",
      "|2008|    7| 31| 805|      991| N562AS|   DEN| SEA|1024|       0|\n",
      "|2008|    4| 27| 736|      989| N557AS|   SFO| SEA| 679|       0|\n",
      "+----+-----+---+----+---------+-------+------+----+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Display the first lines of the database ordered in decreasing order by flight number\n",
    "\n",
    "df.orderBy(df.flightNum.desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7915c1cb-c75e-4ecb-bcbf-29b6ddc53f0a",
   "metadata": {},
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec0afde-1f5c-49db-bccf-58bf2f0c5ddc",
   "metadata": {},
   "source": [
    "df.createOrReplaceTempView(\"people\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87710c63-4b34-45ff-b8a4-335d03f1fc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 90:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|origin|\n",
      "+------+\n",
      "|   SEA|\n",
      "|   SEA|\n",
      "|   SAN|\n",
      "|   SEA|\n",
      "|   TUS|\n",
      "|   LAX|\n",
      "|   LAX|\n",
      "|   ANC|\n",
      "|   LAS|\n",
      "|   SJC|\n",
      "+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 04:28:01 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 90 (TID 94): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating an SQL view\n",
    "df.createOrReplaceTempView(\"flightsView\")\n",
    "\n",
    "# Creating a data frame containing only the variable \"carrierDelay\"\n",
    "sqlDF = spark.sql(\"SELECT origin FROM flightsView\")\n",
    "\n",
    "# Display of the first 10 lines\n",
    "sqlDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a98c6c-9ee1-496d-a34a-1df485168c73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86636081-7e31-4db1-af6e-5dc8e6b313b6",
   "metadata": {},
   "source": [
    "withRemplacement : a Boolean to specify False if you don't want to overwrite the DataFrame\n",
    "fraction : the fraction of data to be kept\n",
    "seed : an integer that allows the results to be reproduced: for the same seed, a function, although random, will always give the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2ae27ed-631b-472c-872a-31015fac7a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>flightNum</th>\n",
       "      <th>tailNum</th>\n",
       "      <th>origin</th>\n",
       "      <th>dest</th>\n",
       "      <th>dist</th>\n",
       "      <th>canceled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>2028</td>\n",
       "      <td>310</td>\n",
       "      <td>N706AS</td>\n",
       "      <td>PDX</td>\n",
       "      <td>SFO</td>\n",
       "      <td>550</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>191</td>\n",
       "      <td>N772AS</td>\n",
       "      <td>ANC</td>\n",
       "      <td>FAI</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>818</td>\n",
       "      <td>31</td>\n",
       "      <td>N784AS</td>\n",
       "      <td>ADQ</td>\n",
       "      <td>ANC</td>\n",
       "      <td>252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>1339</td>\n",
       "      <td>6</td>\n",
       "      <td>N590AS</td>\n",
       "      <td>LAX</td>\n",
       "      <td>DCA</td>\n",
       "      <td>2311</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>2130</td>\n",
       "      <td>313</td>\n",
       "      <td>N564AS</td>\n",
       "      <td>SFO</td>\n",
       "      <td>SEA</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>1034</td>\n",
       "      <td>184</td>\n",
       "      <td>N648AS</td>\n",
       "      <td>FAI</td>\n",
       "      <td>ANC</td>\n",
       "      <td>261</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>1351</td>\n",
       "      <td>62</td>\n",
       "      <td>N783AS</td>\n",
       "      <td>KTN</td>\n",
       "      <td>SEA</td>\n",
       "      <td>680</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>2220</td>\n",
       "      <td>70</td>\n",
       "      <td>N613AS</td>\n",
       "      <td>JNU</td>\n",
       "      <td>SIT</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>2013</td>\n",
       "      <td>614</td>\n",
       "      <td>N317AS</td>\n",
       "      <td>SEA</td>\n",
       "      <td>LAS</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1819</td>\n",
       "      <td>19</td>\n",
       "      <td>N558AS</td>\n",
       "      <td>MCO</td>\n",
       "      <td>SEA</td>\n",
       "      <td>2553</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>628</td>\n",
       "      <td>335</td>\n",
       "      <td>N778AS</td>\n",
       "      <td>SJC</td>\n",
       "      <td>SEA</td>\n",
       "      <td>697</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  month  day  hour  flightNum tailNum origin dest  dist  canceled\n",
       "0   2008      1   30  2028        310  N706AS    PDX  SFO   550         0\n",
       "1   2008      2    1    16        191  N772AS    ANC  FAI   261         0\n",
       "2   2008      3   24   818         31  N784AS    ADQ  ANC   252         0\n",
       "3   2008      3   28  1339          6  N590AS    LAX  DCA  2311         0\n",
       "4   2008      6   15  2130        313  N564AS    SFO  SEA   679         0\n",
       "5   2008      6   22  1034        184  N648AS    FAI  ANC   261         0\n",
       "6   2008      7   15  1351         62  N783AS    KTN  SEA   680         0\n",
       "7   2008      9   30  2220         70  N613AS    JNU  SIT    95         0\n",
       "8   2008     10   22  2013        614  N317AS    SEA  LAS   866         0\n",
       "9   2008     10   25  1819         19  N558AS    MCO  SEA  2553         0\n",
       "10  2008     12    6   628        335  N778AS    SJC  SEA   697         0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display of about ten lines of the database\n",
    "df.sample(False, .00006, seed = 222).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd8522-0a89-4de5-8a2b-5530454a72fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f4503-8311-4a2e-88af-c6481664c97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85d17718-d132-40eb-a737-f1f1dc6691e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a29e67-a3a2-4eae-b661-1ae13a1335db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing SparkContextb\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d139ed-3311-4d03-8263-09344a2bcefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b8777f-e122-4556-86ed-a3f806f59ef9",
   "metadata": {},
   "source": [
    "# Recap Intro DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75aada-2cba-432b-8216-d957bd3a8321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECAP \n",
    "\n",
    "# Importing SparkContext from the pyspark module\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Defining a SparkContext locally\n",
    "sc = SparkContext('local')\n",
    "sc\n",
    "\n",
    "# Definition of a SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Introduction to DataFrame\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n",
    "\n",
    "#Reading the file \"2008_raw.csv\"\n",
    "raw_rdd = sc.textFile(f\"file:///{path}/2008.csv\")\n",
    "\n",
    "\n",
    "# take 5 rows\n",
    "raw_rdd.take(5)\n",
    "\n",
    "\n",
    "# collect all\n",
    "raw_rdd.collect()\n",
    "\n",
    "\n",
    "# count\n",
    "# calculation of the number of lines \n",
    "count = raw_rdd.count()\n",
    "\n",
    "\n",
    "# Creating an rdd whose lines are a list of raw_rdd elements \n",
    "airplane_rdd = raw_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "\n",
    "# map func\n",
    "hist_rdd = airplane_rdd.map(lambda line: (line[7], 1))\n",
    "print(\"hist_rdd = \", hist_rdd.take(3))\n",
    "\n",
    "hist_raw_rdd = raw_rdd.map(lambda line_string: (line_string[0:10], 1))\n",
    "print(\"hist_raw_rdd = \", hist_raw_rdd.take(3))\n",
    "\n",
    "\n",
    "# collect (create list)\n",
    "# Creating a list from an rdd\n",
    "hist = hist_rdd.collect()\n",
    "\n",
    "\n",
    "# sorted\n",
    "# ASC increasing≈ (reverse = 0) # DESC decreasing (reverse = 1) \n",
    "hist_sorted  = sorted(hist, key= lambda x: x[0], reverse= 1)\n",
    "print(hist_rdd[:5])\n",
    "\n",
    "\n",
    "# sortBY\n",
    "hist_sorted_2 = hist.sortBy(lambda couple: couple[1], ascending = True) \\\n",
    "                    .collect()\n",
    "\n",
    "# split \n",
    "# Creating an rdd whose lines are a list of raw_rdd elements \n",
    "airplane_rdd = raw_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "\n",
    "# Calculation and display of the number of cancelled flights by city of origin\n",
    "airplane_rdd \\\n",
    "    .filter(lambda x: x[10] == \"1\") \\\n",
    "    .map(lambda x: (x[8], 1)) \\\n",
    "    .reduceByKey(lambda x,y: x+y) \\\n",
    "    .collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d054e540-4bde-46b2-82a8-0a1a089d2ba3",
   "metadata": {},
   "source": [
    "# Recap ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a60c5ce-7a71-456a-89bd-575962798cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 08:54:43 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| _c0| _c1| _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|_c55|_c56|_c57|_c58|_c59|_c60|_c61|_c62|_c63|_c64|_c65|_c66|_c67|_c68|_c69|_c70|_c71|_c72|_c73|_c74|_c75|_c76|_c77|_c78|_c79|_c80|_c81|_c82|_c83|_c84|_c85|_c86|_c87|_c88|_c89|_c90|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2001|4...|2...|7...|8...|-...|-...|-...|-...|7...|-...|3...|-...|1...|6...|9...|6...|4...|3...|3...|2...|2...|1...|1...|1...|-...|-...|9...|4...|-...|-...|1...|3...|1...|1...|7...|-...|-...|-...|-...|7...|-...|-...|-...|-...|1...|-...|8...|2...|-...|3...|-...|-...|1...|4...|-...|-...|1...|6...|2...|-...|-...|7...|-...|-...|-...|-...|4...|7...|2...|6...|-...|-...|-...|7...|-...|-...|-...|1...|-...|4...|1...|-...|5...|1...|1...|-...|6...|-...|-...|2...|\n",
      "|2001|4...|1...|7...|1...|-...|-...|8...|-...|1...|4...|2...|0...|4...|2...|6...|4...|7...|4...|7...|3...|3...|2...|1...|3...|1...|-...|1...|1...|-...|-...|1...|4...|3...|-...|4...|2...|-...|-...|-...|2...|-...|1...|-...|-...|-...|-...|-...|1...|1...|4...|2...|1...|7...|-...|-...|1...|7...|-...|-...|-...|-...|-...|1...|-...|4...|1...|-...|4...|4...|-...|4...|1...|1...|-...|1...|-...|-...|1...|-...|-...|5...|-...|3...|4...|-...|-...|7...|1...|5...|2...|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: double (nullable = true)\n",
      " |-- _c14: double (nullable = true)\n",
      " |-- _c15: double (nullable = true)\n",
      " |-- _c16: double (nullable = true)\n",
      " |-- _c17: double (nullable = true)\n",
      " |-- _c18: double (nullable = true)\n",
      " |-- _c19: double (nullable = true)\n",
      " |-- _c20: double (nullable = true)\n",
      " |-- _c21: double (nullable = true)\n",
      " |-- _c22: double (nullable = true)\n",
      " |-- _c23: double (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: double (nullable = true)\n",
      " |-- _c31: double (nullable = true)\n",
      " |-- _c32: double (nullable = true)\n",
      " |-- _c33: double (nullable = true)\n",
      " |-- _c34: double (nullable = true)\n",
      " |-- _c35: double (nullable = true)\n",
      " |-- _c36: double (nullable = true)\n",
      " |-- _c37: double (nullable = true)\n",
      " |-- _c38: double (nullable = true)\n",
      " |-- _c39: double (nullable = true)\n",
      " |-- _c40: double (nullable = true)\n",
      " |-- _c41: double (nullable = true)\n",
      " |-- _c42: double (nullable = true)\n",
      " |-- _c43: double (nullable = true)\n",
      " |-- _c44: double (nullable = true)\n",
      " |-- _c45: double (nullable = true)\n",
      " |-- _c46: double (nullable = true)\n",
      " |-- _c47: double (nullable = true)\n",
      " |-- _c48: double (nullable = true)\n",
      " |-- _c49: double (nullable = true)\n",
      " |-- _c50: double (nullable = true)\n",
      " |-- _c51: double (nullable = true)\n",
      " |-- _c52: double (nullable = true)\n",
      " |-- _c53: double (nullable = true)\n",
      " |-- _c54: double (nullable = true)\n",
      " |-- _c55: double (nullable = true)\n",
      " |-- _c56: double (nullable = true)\n",
      " |-- _c57: double (nullable = true)\n",
      " |-- _c58: double (nullable = true)\n",
      " |-- _c59: double (nullable = true)\n",
      " |-- _c60: double (nullable = true)\n",
      " |-- _c61: double (nullable = true)\n",
      " |-- _c62: double (nullable = true)\n",
      " |-- _c63: double (nullable = true)\n",
      " |-- _c64: double (nullable = true)\n",
      " |-- _c65: double (nullable = true)\n",
      " |-- _c66: double (nullable = true)\n",
      " |-- _c67: double (nullable = true)\n",
      " |-- _c68: double (nullable = true)\n",
      " |-- _c69: double (nullable = true)\n",
      " |-- _c70: double (nullable = true)\n",
      " |-- _c71: double (nullable = true)\n",
      " |-- _c72: double (nullable = true)\n",
      " |-- _c73: double (nullable = true)\n",
      " |-- _c74: double (nullable = true)\n",
      " |-- _c75: double (nullable = true)\n",
      " |-- _c76: double (nullable = true)\n",
      " |-- _c77: double (nullable = true)\n",
      " |-- _c78: double (nullable = true)\n",
      " |-- _c79: double (nullable = true)\n",
      " |-- _c80: double (nullable = true)\n",
      " |-- _c81: double (nullable = true)\n",
      " |-- _c82: double (nullable = true)\n",
      " |-- _c83: double (nullable = true)\n",
      " |-- _c84: double (nullable = true)\n",
      " |-- _c85: double (nullable = true)\n",
      " |-- _c86: double (nullable = true)\n",
      " |-- _c87: double (nullable = true)\n",
      " |-- _c88: double (nullable = true)\n",
      " |-- _c89: double (nullable = true)\n",
      " |-- _c90: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 2001|[49.94357,21.4711...|\n",
      "| 2001|[48.73215,18.4293...|\n",
      "| 2001|[50.95714,31.8560...|\n",
      "| 2001|[48.2475,-1.89837...|\n",
      "| 2001|[50.9702,42.20998...|\n",
      "| 2001|[50.54767,0.31568...|\n",
      "| 2001|[50.57546,33.1784...|\n",
      "| 2001|[48.26892,8.97526...|\n",
      "| 2001|[49.75468,33.9958...|\n",
      "| 2007|[45.17809,46.3423...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 08:55:22 WARN Instrumentation: [f08a5bd4] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|        prediction|\n",
      "+-----+--------------------+------------------+\n",
      "| 1922|[40.96435,64.5129...|1995.6636213087854|\n",
      "| 1922|[46.15136,66.0833...|1997.9249712538408|\n",
      "| 1928|[34.74756,-60.680...|1986.8888849610278|\n",
      "| 1928|[37.09369,-119.24...| 1997.332951898663|\n",
      "| 1929|[22.43376,-106.81...|1988.5231716417095|\n",
      "| 1929|[26.64447,-72.858...| 1982.148672732321|\n",
      "| 1929|[28.86846,-134.80...|1991.0562414149865|\n",
      "| 1929|[35.70447,-77.190...|1986.5063135696932|\n",
      "| 1930|[20.82134,-105.06...|1979.7437642435186|\n",
      "| 1930|[34.14566,-89.762...|1985.3130940070123|\n",
      "| 1930|[37.87171,-120.13...| 1988.643392792783|\n",
      "| 1930|[41.08332,-93.379...| 1989.170896188646|\n",
      "| 1931|[36.8584,-91.0008...|1985.1755682183211|\n",
      "| 1931|[36.93633,-84.588...|1981.5020186660693|\n",
      "| 1932|[33.20162,-143.14...|1989.8176878031343|\n",
      "| 1932|[39.96935,-93.586...|1987.8575537621787|\n",
      "| 1934|[38.58574,-107.77...|1987.2084623804262|\n",
      "| 1935|[28.23865,-210.09...| 1994.795950545999|\n",
      "| 1936|[31.87813,-168.63...|1992.0856275527788|\n",
      "| 1937|[31.33392,-150.96...|1988.5575270711975|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "RMSE LinearRegression: 9.551981153490303\n",
      "R2 LinearRegression  :   0.23634168471481043\n",
      "DenseVector([0.873, -0.0566, -0.0436, 0.0053, -0.0153, -0.2203, -0.0075, -0.1007, -0.0711, 0.0248, -0.1637, -0.004, 0.0468, 0.0004, -0.0004, 0.0006, 0.0005, 0.0015, 0.0019, 0.0021, 0.0008, -0.0006, 0.0078, 0.0029, -0.0036, 0.0001, 0.0016, 0.0005, 0.0008, -0.0004, -0.0015, -0.0014, -0.0057, 0.002, 0.0017, -0.0052, -0.0002, 0.0006, 0.0013, -0.0018, -0.0021, -0.0006, -0.0016, -0.0021, -0.0033, 0.0068, 0.0004, -0.0021, 0.0002, 0.0019, 0.0004, -0.0015, 0.002, 0.0006, -0.0002, 0.0002, -0.0019, 0.002, -0.0013, 0.0002, -0.0031, -0.0019, -0.0078, 0.0012, -0.0021, 0.0007, -0.0004, -0.0004, -0.0041, -0.0056, -0.0011, 0.0002, 0.0007, 0.004, 0.0029, 0.0154, 0.0002, -0.0044, -0.0001, -0.0001, -0.0008, -0.0006, 0.0013, 0.001, 0.0267, 0.0001, 0.0012, -0.0317, -0.0015, -0.0015])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 09:00:54 WARN MemoryStore: Not enough space to cache rdd_368_3 in memory! (computed 3.8 MiB so far)\n",
      "23/06/05 09:00:54 WARN BlockManager: Persisting block rdd_368_3 to disk instead.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|        prediction|\n",
      "+-----+--------------------+------------------+\n",
      "| 1922|[40.96435,64.5129...|  1993.38480009605|\n",
      "| 1922|[46.15136,66.0833...|2003.6109438408168|\n",
      "| 1928|[34.74756,-60.680...|  1993.38480009605|\n",
      "| 1928|[37.09369,-119.24...|1996.5106917220153|\n",
      "| 1929|[22.43376,-106.81...|1996.5106917220153|\n",
      "| 1929|[26.64447,-72.858...|1996.8444276695031|\n",
      "| 1929|[28.86846,-134.80...|1996.8444276695031|\n",
      "| 1929|[35.70447,-77.190...|1989.6429693424332|\n",
      "| 1930|[20.82134,-105.06...|1996.5106917220153|\n",
      "| 1930|[34.14566,-89.762...|1989.6429693424332|\n",
      "+-----+--------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE DecisionTreeRegressor: 9.988742893187132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 134:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared DecisionTreeRegressor : 0.16524223180966502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importing SparkSession and SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "# Defining a SparkContext locally\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "# Building a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction to Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n",
    "\n",
    "pathSD = \"/Volumes/SD512/indirilenler_SD_Card/data\"\n",
    "\n",
    "# Loading the file \"YearPredictionMSD.txt\" into a data frame\n",
    "df_raw = spark.read.csv(f'file:///{pathSD}/YearPredictionMSD.txt')\n",
    "\n",
    "\n",
    "# First display method\n",
    "df_raw.show(2, truncate = 4)\n",
    "# By modifying the values of 'truncate', this method does not allow a good visualization of the data # by the number of variables\n",
    "\n",
    "# Second display method\n",
    "df_raw.sample(False, .00001, seed = 222).toPandas()\n",
    "# Using toPandas allows you to better visualize the data\n",
    "\n",
    "\n",
    "# Import of col from package pyspark.sql.functions\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Convert columns related to timbre to double and year to int\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[1:91]]\n",
    "df = df_raw.select(df_raw._c0.cast('int'), *exprs)\n",
    "\n",
    "\n",
    "# display of the variable schema\n",
    "df.printSchema()\n",
    "\n",
    "\n",
    "# -------------- LinearRegression ---------------------\n",
    "# Importing DenseVector from the package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Creating an rdd by separating the variable to be explained from the features\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Creation of a data frame composed of two variables: label and features\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "# Display of the first 10 lines of the data frame\n",
    "df_ml.show(10)\n",
    "\n",
    "# Splitting the data into two training and test sets ==> by default the sample is randomly distributed\n",
    "train, test = df_ml.randomSplit([.8, .2], seed= 1234)\n",
    "\n",
    "# Import of LinearRegression of the package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Creating a linear regression function\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features')\n",
    "\n",
    "# Fitting of training data \"train\".\n",
    "linearModel = lr.fit(train)\n",
    "\n",
    "# Calculation of test data predictions\n",
    "predicted = linearModel.transform(test)\n",
    "\n",
    "# Printing prediction\n",
    "predicted.show()\n",
    "\n",
    "# Calculation and display of the RMSE\n",
    "print(\"RMSE LinearRegression:\", linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Calculation and display of R2\n",
    "print(\"R2 LinearRegression  :  \", linearModel.summary.r2)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Displaying the Coefficients of the linear model\n",
    "pprint(linearModel.coefficients)\n",
    "\n",
    "\n",
    "# -------------- DecisionTreeRegressor ---------------------\n",
    "# Import of DecisionTreeRegressor of the package pyspark.ml.regression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Creating a linear regression function\n",
    "dt = DecisionTreeRegressor(labelCol='label', featuresCol= 'features')\n",
    "\n",
    "# Fitting of training data \"train\".\n",
    "decisionTreeModel = dt.fit(train)\n",
    "\n",
    "# Calculation of test data predictions\n",
    "predicted = decisionTreeModel.transform(test)\n",
    "\n",
    "# Printing prediction\n",
    "predicted.show(10)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Creating a regression evaluator\n",
    "evaluatorRmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
    "\n",
    "# Calculating RMSE\n",
    "rmse = evaluatorRmse.evaluate(predicted)\n",
    "\n",
    "# Printing RMSE = Root Mean Squared Error\n",
    "print(\"RMSE DecisionTreeRegressor:\", rmse)\n",
    "\n",
    "# Creating a regression evaluator for R-squared\n",
    "evaluatorR2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')\n",
    "\n",
    "# Calculating R-squared\n",
    "r2 = evaluatorR2.evaluate(predicted)\n",
    "\n",
    "# Printing R-squared\n",
    "print(\"R-squared DecisionTreeRegressor :\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1ed6b7-dd25-4da5-b0cb-accf7ae0d5e9",
   "metadata": {},
   "source": [
    "# Recap Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c759d0f-bba5-4174-aa43-bce6180b1bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/05 10:56:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/05 10:57:04 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- StringIndexer ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Reconstructed column ----------------------\n",
      "---------------------- Pipeline ----------------------\n",
      "---------------------- svmLib ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- RandomForestClassifier ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Evaluator ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier Accuracy =  0.963758389261745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Recap Model Evaluation\n",
    "\n",
    "# Importing SparkSession and SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Defining a SparkContext locally\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Building a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipelines Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark\n",
    "\n",
    "pathSD = \"/Volumes/SD512/indirilenler_SD_Card/data\"\n",
    "\n",
    "# Loading the file 'HR_comma_sep.csv'\n",
    "hr = spark.read.csv(f'file:///{pathSD}/HR_comma_sep.csv', header = True)\n",
    "\n",
    "hr = hr.withColumnRenamed(\"Department\", \"sales\")\n",
    "\n",
    "# Displaying an extract from the data frame\n",
    "hr.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "# Sort the variables in such a way that the label is in the first column\n",
    "hr = hr.select( 'left',\n",
    "               'satisfaction_level',\n",
    "               'last_evaluation',\n",
    "               'number_project',\n",
    "               'average_montly_hours',\n",
    "               'time_spend_company',\n",
    "               'Work_accident',\n",
    "               'promotion_last_5years',\n",
    "               'sales',\n",
    "               'salary')\n",
    "\n",
    "# Display of a description of the variables\n",
    "hr.describe().toPandas()\n",
    "\n",
    "print(\"---------------------- StringIndexer ----------------------\")\n",
    "# ---------------------- StringIndexer ----------------------\n",
    "# Importing StringIndexer from the package pyspark.ml.feature\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Creation of an indexer transforming a dirty variable into indexedSales\n",
    "salesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales').fit(hr)\n",
    "\n",
    "# Creating a DataFrame hrSalesIndexed indexing the variable sales\n",
    "hrSalesIndexed = salesIndexer.transform(hr)\n",
    "\n",
    "# Displaying an extract from the hrSalesIndexed data frame \n",
    "hrSalesIndexed.sample(False, 0.0004 , seed = 222).toPandas()\n",
    "\n",
    "# Import of IndexToString from the pyspark.ml.feature package\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "print(\"---------------------- Reconstructed column ----------------------\")\n",
    "# ---------------------- Reconstructed column ----------------------\n",
    "# Creating a new salesReconstructed column\n",
    "SalesReconstructor = IndexToString(inputCol='indexedSales',\n",
    "                                   outputCol='salesReconstructed',\n",
    "                                   labels = salesIndexer.labels)\n",
    "\n",
    "# Apply the SalesReconstructor transformer\n",
    "hrSalesReconstructed = SalesReconstructor.transform(hrSalesIndexed)\n",
    "\n",
    "# Displaying an extract from the database\n",
    "hrSalesReconstructed.sample(False, 0.0004 , seed = 222).toPandas()\n",
    "# A new column \"salesReconstructed\" equal to the column \"sales\" appears\n",
    "\n",
    "\n",
    "# Pipeline import from pyspark.ml package\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "print(\"---------------------- Pipeline ----------------------\")\n",
    "# ---------------------- Pipeline ----------------------\n",
    "# Creation of indexers\n",
    "SalesIndexer = StringIndexer(inputCol='sales', outputCol='indexedSales')\n",
    "SalaryIndexer = StringIndexer(inputCol='salary', outputCol='indexedSalary')\n",
    "\n",
    "# Creating a pipeline\n",
    "indexer = Pipeline(stages =  [SalaryIndexer, SalesIndexer])\n",
    "\n",
    "# Index the variables of \"hr\"\n",
    "hrIndexed = indexer.fit(hr).transform(hr)\n",
    "\n",
    "# Displaying an extract\n",
    "hrIndexed.sample(False, 0.0004 , seed = 222).toPandas()\n",
    "\n",
    "print(\"---------------------- svmLib ----------------------\")\n",
    "# ---------------------- svmLib ----------------------\n",
    "# Import of DenseVector from the pyspark.ml.linalg package\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Creation of a database excluding non-indexed variables\n",
    "hrNumeric = hrIndexed.select('left',\n",
    "                             'satisfaction_level',\n",
    "                             'last_evaluation',\n",
    "                             'number_project',\n",
    "                             'average_montly_hours',\n",
    "                             'time_spend_company',\n",
    "                             'Work_accident',\n",
    "                             'promotion_last_5years',\n",
    "                             'indexedSales',\n",
    "                             'indexedSalary')\n",
    "\n",
    "# Creation of a DenseVector variable containing the features via the RDD structure\n",
    "hrRdd = hrNumeric.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# transformation into DataFrame and naming variables to get a base of the form libsvm\n",
    "hrLibsvm = spark.createDataFrame(hrRdd, ['label', 'features'])\n",
    "\n",
    "# Displaying an extract\n",
    "hrLibsvm.sample(False, .0004, seed = 222).toPandas()\n",
    "\n",
    "print(\"---------------------- RandomForestClassifier ----------------------\")\n",
    "# ---------------------- RandomForestClassifier ----------------------\n",
    "# Import of the RandomForestClassifier from the pyspark.ml.classification package\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Import of VectorIndexer from the pyspark.ml.feature package\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# Creation of transformers\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(hrLibsvm)\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories = 10).fit(hrLibsvm)\n",
    "\n",
    "# Creation of a classifier \n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", predictionCol='prediction', seed = 222)\n",
    "\n",
    "# Creation of a transformer to restore the labels of predictions\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Creating a Pipeline \n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "\n",
    "# Splitting data into two sets: training and test \n",
    "(train, test) = hrLibsvm.randomSplit([0.7, 0.3], seed = 222)\n",
    "\n",
    "# Training the model using the training data\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "#Calculation of predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Display of an extract of the predictions  \n",
    "predictions.sample(False, 0.001 , seed = 222).toPandas()\n",
    "\n",
    "print(\"---------------------- Evaluator ----------------------\")\n",
    "# ---------------------- Evaluator ----------------------\n",
    "# Import of a MulticlassClassificationEvaluator evaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Creation of an evaluator \n",
    "evaluator = MulticlassClassificationEvaluator(metricName='accuracy',\n",
    "                                              labelCol= 'indexedLabel',\n",
    "                                              predictionCol= 'prediction')\n",
    "# Calculation and display of model accuracy \n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"RandomForestClassifier Accuracy = \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57877a9b-abfc-4e65-b7bf-7ee711fadfb8",
   "metadata": {},
   "source": [
    "# Recap Model Tunnig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29990fa5-46bd-4a77-b262-db1349d079fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/05 12:22:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      "\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 2001|[50.57546,33.1784...|\n",
      "| 2003|[37.68491,-26.841...|\n",
      "| 2000|[41.37983,0.14209...|\n",
      "| 2000|[50.86288,56.5713...|\n",
      "| 1998|[43.55369,-8.5369...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "---------------------- Train Test ----------------------\n",
      "---------------------- LinearRegression ----------------------\n",
      "---------------------- Param Grid Builder ----------------------\n",
      "---------------------- RegressionEvaluator ----------------------\n",
      "---------------------- CrossValidator ----------------------\n",
      "---------------------- model.fit(train) ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/05 12:22:59 WARN Instrumentation: [ae4c052e] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:23:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/06/05 12:23:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n",
      "23/06/05 12:23:09 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "23/06/05 12:23:19 WARN Instrumentation: [a360f3b9] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:23:20 WARN Instrumentation: [4176b54b] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:23:30 WARN Instrumentation: [4ec876dd] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:23:46 WARN Instrumentation: [9480fd40] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:23:47 WARN Instrumentation: [8f83bc98] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:23:54 WARN Instrumentation: [5bb40eeb] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:24:10 WARN Instrumentation: [e2baae82] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:24:11 WARN Instrumentation: [2dbdba30] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/06/05 12:24:27 WARN Instrumentation: [049501b0] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 112.721 seconds\n",
      "---------------------- predictions ----------------------\n",
      "---------------------- r2, rmse ----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_Square =  0.1507749053877322\n",
      "RMSE =  9.934428134135151\n",
      "---------------------- rmodel coefficients ----------------------\n",
      "[0.7398414243248653,-0.05660403223973706,-0.07564839462492409,0.13904782281618258,0.014625527053163135,-0.21347108350347607,-0.06160659876036667,-0.05335451779315963,-0.12181883863897003,0.0997640090763514,-0.44761935778485695,0.02607668980106921]\n",
      "Total Time =  167.3572597503662\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "\n",
    "# Importing SparkSession and SParkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Creating a SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Creating a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML Tuning\") \\\n",
    "    .getOrCreate()\n",
    "        \n",
    "spark\n",
    "\n",
    "\n",
    "pathSD = \"/Volumes/SD512/indirilenler_SD_Card/data\"\n",
    "\n",
    "# Loading the file \"YearPredictionMSD.txt\" into a data frame\n",
    "df_full = spark.read.csv(f'file:///{pathSD}/YearPredictionMSD.txt', header=False)\n",
    "\n",
    "# We infer the right types from the columns\n",
    "from pyspark.sql.functions import col\n",
    "exprs = [col(c).cast(\"double\") for c in df_full.columns[1:13]]\n",
    "\n",
    "df_casted = df_full.select(df_full._c0.cast('int'),\n",
    "                           *exprs)\n",
    "\n",
    "# Finally, for the sake of speed of calculations,\n",
    "# we will only process an extract from the database in this exercise\n",
    "df = df_casted.sample(False, .1, seed = 222)\n",
    "\n",
    "df.sample(False, .0001, seed = 222).toPandas()\n",
    "\n",
    "print(df.printSchema())\n",
    "\n",
    "# ---------------------- svmLib ----------------------\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Conversion of the database to svmlib format\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "df_ml.show(5)\n",
    "\n",
    "print(\"---------------------- Train Test ----------------------\")\n",
    "\n",
    "# Splitting data into two training and test sets\n",
    "# by default the sample is randomly distributed\n",
    "train, test = df_ml.randomSplit([0.8, 0.2], seed=222)\n",
    "\n",
    "\n",
    "print(\"---------------------- LinearRegression ----------------------\")\n",
    "# import of LinearRegression from the pyspark.ml.regression package\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Creating an estimator: Linear regression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "print(\"---------------------- Param Grid Builder ----------------------\")\n",
    "# Import of ParamGridBuilder from the pyspark.ml.tuning package\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# Creating a parameter grid\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0, 0.5, 1]).\\\n",
    "    addGrid(lr.elasticNetParam, [0, 0.5, 1]).\\\n",
    "    build()\n",
    "\n",
    "print(\"---------------------- RegressionEvaluator ----------------------\")\n",
    "# Import of RegressionEvaluator from the pyspark.ml.evaluation package\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "ev = RegressionEvaluator(predictionCol='prediction',\n",
    "                                labelCol='label',\n",
    "                                metricName='rmse')\n",
    "\n",
    "print(\"---------------------- CrossValidator ----------------------\")\n",
    "#Import of CrossValidator from the pyspark.ml.tuning package\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Creation of a 3-fold cross validator\n",
    "cv = CrossValidator(estimator = lr,\n",
    "                    estimatorParamMaps = param_grid,\n",
    "                    evaluator=ev,\n",
    "                    numFolds=3)\n",
    "\n",
    "print(\"---------------------- model.fit(train) ----------------------\")\n",
    "# Import of the time library and calculation of time at the beginning of the execution (t0)\n",
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "#lr = LinearRegression(featuresCol = 'features', labelCol = 'label')\n",
    "#ev = RegressionEvaluator(predictionCol='prediction', labelCol='label', metricName='r2')\n",
    "#cv = CrossValidator(estimator = lr, estimatorParamMaps = param_grid, evaluator = ev, numFolds = 3)\n",
    "\n",
    "cv_model = cv.fit(train)\n",
    "\n",
    "tt = time() - t0\n",
    "print(f\"Done in {round(tt, 3)} seconds\")\n",
    "\n",
    "print(\"---------------------- predictions ----------------------\")\n",
    "# Calculation of predictions on training set \n",
    "pred_train = cv_model.transform(train)\n",
    "\n",
    "# Calculation of predictions on test set \n",
    "pred_test  = cv_model.transform(test)\n",
    "\n",
    "print(\"---------------------- r2, rmse ----------------------\")\n",
    "r_square = ev.setMetricName('r2').evaluate(pred_test)\n",
    "\n",
    "rmse = ev.setMetricName('rmse').evaluate(pred_test)\n",
    "\n",
    "print(\"R_Square = \",r_square)\n",
    "print(\"RMSE = \",rmse)\n",
    "\n",
    "print(\"---------------------- rmodel coefficients ----------------------\")\n",
    "# Display of model coefficients\n",
    "print(cv_model.bestModel.coefficients)\n",
    "\n",
    "\n",
    "# Total Time\n",
    "finish = time()\n",
    "print(\"Total Time = \", finish - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e05f9ad-5a36-46f9-a3f5-58db86ca87ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (Visual Studio)",
   "language": "python",
   "name": "visualstudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
